#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Mock DiffSinger FastAPI Server with Edge TTS Integration

Microsoft Edge TTSã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ãªæ—¥æœ¬èªéŸ³å£°åˆæˆã‚’è¡Œã†ãƒ¢ãƒƒã‚¯ã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚
æ•°å­¦çš„åˆæˆã®ä»£ã‚ã‚Šã«ã€å®Ÿéš›ã®éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦è‡ªç„¶ãªæ­Œå£°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

èµ·å‹•æ–¹æ³•:
    python enhanced_mock_diffsinger_server.py

APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
    POST /api/synthesize
    GET /health
    GET /api/models
"""

import sys
import os
import io
import json
import time
import asyncio
import tempfile
from pathlib import Path
from typing import Optional

# Windowsç’°å¢ƒã§UTF-8å‡ºåŠ›ã‚’å¼·åˆ¶
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field
import uvicorn

# Edge TTS import
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
    print("[Enhanced Mock] Edge TTS successfully imported")
except ImportError:
    EDGE_TTS_AVAILABLE = False
    print("[Enhanced Mock] Edge TTS not available, falling back to mathematical synthesis")

# Pydub import for audio format conversion
try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
    print("[Enhanced Mock] Pydub successfully imported for audio conversion")
except ImportError:
    PYDUB_AVAILABLE = False
    print("[Enhanced Mock] Pydub not available, MP3 to WAV conversion will use fallback")

# Musical synthesis imports
import numpy as np
import wave
import struct
import math

# Musical synthesis configuration
SAMPLE_RATE = 44100  # CD quality sample rate
MUSICAL_NOTE_DURATION = 1.0  # 1.0 second per note for appropriate rhythm
FEMALE_VOICE_PITCH_BOOST = 12  # +12 semitones for more feminine voice

# åˆæˆãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ–¹å¼ã®è¿½åŠ ï¼‰
SYNTHESIS_MODE = "musical_first"  # æ•°å­¦çš„åˆæˆã‚·ã‚¹ãƒ†ãƒ ã«æˆ»ã™
# Options: musical_first, edge_tts_post, edge_tts_only

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="Enhanced Mock DiffSinger API",
    description="Edge TTSçµ±åˆãƒªã‚¢ãƒ«æ—¥æœ¬èªæ­Œå£°åˆæˆã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰",
    version="2.0.0"
)

# CORSè¨­å®šï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:5174",
        "http://localhost:5175",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:5174",
        "http://127.0.0.1:5175"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

class SynthesisRequest(BaseModel):
    """åˆæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«"""
    lyrics: str = Field(..., min_length=1, max_length=1000, description="æ­Œè©ï¼ˆä¸­å›½èªãƒ»æ—¥æœ¬èªï¼‰")
    notes: str = Field(..., description="MIDIéŸ³åï¼ˆ|åŒºåˆ‡ã‚Šï¼‰ä¾‹: C4 | D4 | E4")
    durations: str = Field(..., description="ãƒãƒ¼ãƒˆé•·ã•ç§’ï¼ˆ|åŒºåˆ‡ã‚Šï¼‰ä¾‹: 0.5 | 0.5 | 1.0")
    output_path: str = Field(default="outputs/synthesis.wav", description="å‡ºåŠ›WAVãƒ‘ã‚¹")

class SynthesisResponse(BaseModel):
    """åˆæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«"""
    status: str
    message: str
    audio_path: str
    duration: float

class ModelInfo(BaseModel):
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±"""
    id: str
    name: str
    language: str
    description: str

# ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆEdge TTSçµ±åˆç‰ˆï¼‰
MOCK_MODELS = [
    ModelInfo(
        id="edge_tts_nanami",
        name="Edge TTS Nanami (Japanese Female)",
        language="ja_JP",
        description="Microsoft Edge TTS Nanami Neural - æ—¥æœ¬èªå¥³æ€§éŸ³å£°ï¼ˆé«˜å“è³ªãƒ»è‡ªç„¶ï¼‰"
    ),
    ModelInfo(
        id="edge_tts_keita",
        name="Edge TTS Keita (Japanese Male)",
        language="ja_JP",
        description="Microsoft Edge TTS Keita Neural - æ—¥æœ¬èªç”·æ€§éŸ³å£°ï¼ˆé«˜å“è³ªãƒ»è‡ªç„¶ï¼‰"
    ),
    ModelInfo(
        id="mathematical_fallback",
        name="Mathematical Synthesis (Fallback)",
        language="ja_JP",
        description="æ•°å­¦çš„éŸ³å£°åˆæˆï¼ˆEdge TTSåˆ©ç”¨ä¸å¯æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
    )
]

current_model = "edge_tts_nanami"

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    print("=" * 70)
    print("Enhanced Mock DiffSinger Server Starting...")
    print("=" * 70)
    if EDGE_TTS_AVAILABLE:
        print("[OK] Edge TTS successfully loaded")
        print("[OK] Japanese voices: Nanami (Female), Keita (Male)")
        print("[OK] Real Japanese speech synthesis enabled")
    else:
        print("[WARN] Edge TTS not available - falling back to mathematical synthesis")
    print("[OK] Enhanced Mock DiffSinger Engine initialized successfully")
    print("[OK] Server ready at http://localhost:8001")
    print("[OK] API docs at http://localhost:8001/docs")
    print("=" * 70)

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "service": "Enhanced Mock DiffSinger API",
        "version": "2.0.0",
        "status": "running",
        "edge_tts_available": EDGE_TTS_AVAILABLE,
        "current_model": current_model,
        "docs": "/docs",
        "note": "This is an enhanced mock server with Edge TTS integration"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "service": "Enhanced Mock DiffSinger",
        "version": "2.0.0",
        "current_model": current_model,
        "edge_tts_available": EDGE_TTS_AVAILABLE,
        "timestamp": time.time()
    }

@app.get("/api/models")
async def get_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§"""
    return {
        "models": [model.dict() for model in MOCK_MODELS],
        "current": current_model,
        "total": len(MOCK_MODELS),
        "edge_tts_enabled": EDGE_TTS_AVAILABLE
    }

@app.post("/api/models/{model_id}/load")
async def load_model(model_id: str):
    """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
    global current_model

    # ãƒ¢ãƒ‡ãƒ«IDã®æ¤œè¨¼
    model_ids = [model.id for model in MOCK_MODELS]
    if model_id not in model_ids:
        raise HTTPException(
            status_code=404,
            detail=f"Model {model_id} not found. Available models: {model_ids}"
        )

    # ãƒ¢ãƒƒã‚¯é…å»¶ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    await asyncio.sleep(0.5)

    previous_model = current_model
    current_model = model_id
    return {
        "status": "success",
        "message": f"Model {model_id} loaded successfully",
        "previous_model": previous_model,
        "current_model": model_id,
        "edge_tts_enabled": EDGE_TTS_AVAILABLE
    }

def create_ssml_with_pitch(text: str, notes_list: list, durations_list: list) -> str:
    """æ­Œè©ã¨MIDIãƒãƒ¼ãƒˆæƒ…å ±ã‹ã‚‰SSMLã‚’ç”Ÿæˆ"""

    def midi_note_to_pitch_percent(note_name: str) -> str:
        """MIDIãƒãƒ¼ãƒˆåã‚’ãƒ”ãƒƒãƒãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ›"""
        # C4 (MIDI 60) ã‚’åŸºæº– (0%) ã¨ã™ã‚‹
        note_to_midi = {
            'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3, 'E': 4,
            'F': 5, 'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8, 'Ab': 8, 'A': 9,
            'A#': 10, 'Bb': 10, 'B': 11
        }

        try:
            # éŸ³åã¨ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ã‚’åˆ†é›¢ (ä¾‹: C4 â†’ C, 4)
            if '#' in note_name or 'b' in note_name:
                note = note_name[:-1]
                octave = int(note_name[-1])
            else:
                note = note_name[:-1]
                octave = int(note_name[-1])

            # MIDIãƒãƒ¼ãƒˆç•ªå·ã‚’è¨ˆç®—
            midi_number = octave * 12 + note_to_midi.get(note, 0)

            # C4 (MIDI 60) ã‹ã‚‰ã®å·®åˆ†ã‚’ã‚»ãƒŸãƒˆãƒ¼ãƒ³æ•°ã§è¨ˆç®—
            semitone_diff = midi_number - 60

            # ã‚»ãƒŸãƒˆãƒ¼ãƒ³ã‚’ãƒ”ãƒƒãƒãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ› (1ã‚»ãƒŸãƒˆãƒ¼ãƒ³ â‰ˆ 5.946%)
            pitch_percent = semitone_diff * 5.946

            return f"{pitch_percent:+.1f}%"

        except (ValueError, KeyError):
            return "+0.0%"  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def duration_to_rate(duration: float) -> str:
        """ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç§’ï¼‰ã‚’ç™ºè©±é€Ÿåº¦ã«å¤‰æ›"""
        if duration < 0.4:
            return "fast"
        elif duration < 0.8:
            return "medium"
        else:
            return "slow"

    # æ­Œè©ã‚’æ–‡å­—å˜ä½ã§åˆ†å‰²
    lyrics_chars = list(text)

    # ãƒãƒ¼ãƒˆæ•°ã¨æ­Œè©æ–‡å­—æ•°ã‚’èª¿æ•´
    if len(lyrics_chars) != len(notes_list):
        if len(lyrics_chars) < len(notes_list):
            # æ­Œè©ãŒå°‘ãªã„å ´åˆã¯æœ€å¾Œã®æ–‡å­—ã‚’å»¶é•·
            while len(lyrics_chars) < len(notes_list):
                lyrics_chars.append(lyrics_chars[-1] if lyrics_chars else 'ã‚')
        else:
            # æ­Œè©ãŒå¤šã„å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            lyrics_chars = lyrics_chars[:len(notes_list)]

    # ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‚‚èª¿æ•´
    durations_float = [float(d) for d in durations_list[:len(lyrics_chars)]]

    # SSMLè¦ç´ ã‚’ç”Ÿæˆ
    prosody_elements = []
    for char, note, duration in zip(lyrics_chars, notes_list, durations_float):
        pitch = midi_note_to_pitch_percent(note)
        rate = duration_to_rate(duration)
        prosody_elements.append(f'<prosody pitch="{pitch}" rate="{rate}">{char}</prosody>')

    # å®Œå…¨ãªSSMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ§‹ç¯‰
    ssml_content = ''.join(prosody_elements)
    ssml_document = f'''<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="ja-JP">
{ssml_content}
</speak>'''

    print(f"[SSML] Generated SSML for '{text}' with {len(notes_list)} notes")
    print(f"[SSML] Sample: {prosody_elements[0] if prosody_elements else 'No elements'}")

    return ssml_document

async def convert_lyrics_to_japanese_phonetics(lyrics: str) -> str:
    """æ­Œè©ã‚’æ—¥æœ¬èªç™ºéŸ³ã«æœ€é©åŒ–"""

    # ğŸµ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: ã€Œã‚ã€ã‚’ã€Œã‚ã‚ŠãŒã¨ã†ã€ã“ã“ã‚ã‹ã‚‰ã€ã«å¤‰æ›
    if lyrics.strip() == "ã‚":
        # 20ãƒãƒ¼ãƒˆç”¨ã®ã€Œã‚ã‚ŠãŒã¨ã†ã€ã“ã“ã‚ã‹ã‚‰ã€ã‚’2å›ç¹°ã‚Šè¿”ã—
        return "ã‚ ã‚Š ãŒ ã¨ ã† ã“ ã“ ã‚ ã‹ ã‚‰ ã‚ ã‚Š ãŒ ã¨ ã† ã“ ã“ ã‚ ã‹ ã‚‰"

    # ä¸­å›½èªéŸ³ã«è¿‘ã„æ­Œè©ã‚’æ—¥æœ¬èªèª­ã¿ã«å¤‰æ›
    chinese_to_japanese_map = {
        "å¡": "ã‹",
        "çˆ±": "ã‚ã„",
        "é²": "ã‚‹",
        "è¯º": "ã®",
        "ä¹Œ": "ã†",
        "ä»–": "ãŸ",
        "å˜": "ãŒ"
    }

    result = lyrics
    for chinese, japanese in chinese_to_japanese_map.items():
        result = result.replace(chinese, japanese)

    # æ­Œã„ã‚„ã™ãã™ã‚‹ãŸã‚ã®èª¿æ•´
    result = result.replace("ã‚ã„", "ã‚")  # æ­Œã§ã¯é•·ã„æ¯éŸ³ã‚’çŸ­ç¸®

    return result

async def generate_edge_tts_with_fixed_ssml(text: str, voice: str, output_path: str, notes_list: list, durations_list: list) -> bool:
    """Edge TTSä¿®æ­£ç‰ˆ - æ­£ã—ã„SSMLå½¢å¼ã§ãƒ”ãƒƒãƒåˆ¶å¾¡"""
    try:
        voice_mapping = {
            "edge_tts_nanami": "ja-JP-NanamiNeural",
            "edge_tts_keita": "ja-JP-KeitaNeural"
        }

        edge_voice = voice_mapping.get(voice, "ja-JP-NanamiNeural")
        print(f"[Fixed SSML] Generating speech with voice: {edge_voice}")
        print(f"[Fixed SSML] Text: {text}")
        print(f"[Fixed SSML] Notes: {notes_list}")

        # ãƒ”ãƒƒãƒã¨é€Ÿåº¦ã‚’ç›´æ¥è¨ˆç®—ï¼ˆå¼•æ•°ãƒ™ãƒ¼ã‚¹TTSåˆ¶å¾¡ï¼‰
        if not notes_list or len(notes_list) == 0:
            pitch_percentage = 0
            rate = "medium"
        else:
            # æœ€åˆã®ãƒãƒ¼ãƒˆã®éŸ³ç¨‹ã‚’ä½¿ç”¨ï¼ˆå˜ä¸€ãƒ”ãƒƒãƒåˆ¶å¾¡ï¼‰
            first_note = notes_list[0]

            # MIDIéŸ³åã‚’A4ã‹ã‚‰ã®åŠéŸ³æ•°ã«å¤‰æ›
            note_frequencies = {
                'C': -9, 'C#': -8, 'Db': -8, 'D': -7, 'D#': -6, 'Eb': -6, 'E': -5,
                'F': -4, 'F#': -3, 'Gb': -3, 'G': -2, 'G#': -1, 'Ab': -1, 'A': 0,
                'A#': 1, 'Bb': 1, 'B': 2
            }

            try:
                # éŸ³åã¨ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ç•ªå·ã‚’åˆ†é›¢
                if '#' in first_note or 'b' in first_note:
                    note = first_note[:-1]
                    octave = int(first_note[-1])
                else:
                    note = first_note[:-1]
                    octave = int(first_note[-1])

                # A4ã‹ã‚‰ã®åŠéŸ³æ•°ã‚’è¨ˆç®—
                base_octave = 4
                semitones_from_a4 = (octave - base_octave) * 12 + note_frequencies.get(note, 0)
            except (ValueError, KeyError):
                print(f"[Argument TTS] Warning: Invalid note '{first_note}', using default")
                semitones_from_a4 = 0  # A4ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            # ã‚ˆã‚Šé©åˆ‡ãªãƒ”ãƒƒãƒç¯„å›²ï¼ˆå¥³æ€§ã®å£°ç”¨ã«èª¿æ•´ï¼‰+ éŸ³éšã‚’æ˜ç¢ºã«ã™ã‚‹
            base_pitch_multiplier = 10  # ã‚ˆã‚Šæ˜ç¢ºãªéŸ³éšå¤‰åŒ–
            feminine_pitch_offset = FEMALE_VOICE_PITCH_BOOST * base_pitch_multiplier  # å¥³æ€§ã‚‰ã—ã„é«˜éŸ³
            pitch_percentage = max(-50, min(80, semitones_from_a4 * base_pitch_multiplier + feminine_pitch_offset))

            # ç·æ™‚é–“ã«åŸºã¥ãèª­ã¿ä¸Šã’é€Ÿåº¦ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ã«èª¿æ•´ï¼‰
            total_duration = sum(durations_list)
            if total_duration < 3:
                rate = "+20%"  # é€Ÿã‚
            elif total_duration > 15:
                rate = "-50%"  # éå¸¸ã«é…ã‚ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ï¼‰
            elif total_duration > 8:
                rate = "-40%"  # é…ã‚ï¼ˆæ”¹å–„ç‰ˆï¼‰
            else:
                rate = "-20%"  # ã‚„ã‚„é…ã‚ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’èª¿æ•´ï¼‰

        # ãƒ”ãƒƒãƒã‚’Hzå½¢å¼ã«å¤‰æ›ï¼ˆEdge TTSè¦æ±‚ä»•æ§˜ï¼‰
        pitch_hz = int(pitch_percentage * 2)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’Hzã«å¤‰æ›
        pitch_value = f"{pitch_hz:+d}Hz"
        print(f"[Argument-based TTS] Using Edge TTS argument control: pitch={pitch_value}, rate={rate}")

        # Communicateã®å¼•æ•°ã‚’å‹•çš„ã«æ§‹ç¯‰
        communicate_args = {
            'text': text,
            'voice': edge_voice,
            'pitch': pitch_value
        }
        if rate is not None:
            communicate_args['rate'] = rate

        communicate = edge_tts.Communicate(**communicate_args)

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_path = output_path + "_temp.mp3"
        await communicate.save(temp_path)

        if not os.path.exists(temp_path):
            print(f"[Fixed SSML] Error: Temporary file not created: {temp_path}")
            return False

        # MP3ã‹ã‚‰WAVã«å¤‰æ›
        conversion_success = convert_mp3_to_wav(temp_path, output_path)
        if not conversion_success:
            print(f"[Fixed SSML] Warning: MP3 to WAV conversion failed for {output_path}")
            return False

        print(f"[Fixed SSML] Musical audio generated successfully: {output_path}")
        return True

    except Exception as e:
        print(f"[Fixed SSML] Error generating audio: {e}")
        return False

def convert_mp3_to_wav(mp3_path: str, wav_path: str) -> bool:
    """MP3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›ã™ã‚‹"""
    try:
        if PYDUB_AVAILABLE:
            # Pydubã‚’ä½¿ç”¨ã—ãŸé«˜å“è³ªå¤‰æ›
            print(f"[Audio Converter] Converting {mp3_path} to {wav_path} using Pydub")
            audio = AudioSegment.from_mp3(mp3_path)

            # WAVå½¢å¼ã§å‡ºåŠ›ï¼ˆ16bit, ã‚¹ãƒ†ãƒ¬ã‚ª, 44.1kHzï¼‰
            audio.export(wav_path, format="wav", parameters=[
                "-ar", "44100",  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                "-ac", "2",      # ã‚¹ãƒ†ãƒ¬ã‚ª
                "-sample_fmt", "s16"  # 16bit
            ])

            # å…ƒã®MP3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(mp3_path):
                os.remove(mp3_path)
                print(f"[Audio Converter] Removed temporary MP3 file: {mp3_path}")

            print(f"[Audio Converter] Successfully converted to WAV: {wav_path}")
            return True

        else:
            # PydubãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(f"[Audio Converter] Pydub not available, using file rename fallback")
            os.rename(mp3_path, wav_path)
            print(f"[Audio Converter] Fallback: Renamed {mp3_path} to {wav_path}")
            return True

    except Exception as e:
        print(f"[Audio Converter] Conversion failed: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒãƒ¼ãƒ ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            os.rename(mp3_path, wav_path)
            print(f"[Audio Converter] Error fallback: Renamed {mp3_path} to {wav_path}")
            return True
        except Exception as rename_error:
            print(f"[Audio Converter] Rename fallback also failed: {rename_error}")
            return False

def midi_note_to_frequency(note_name: str) -> float:
    """MIDIéŸ³åã‚’æ­£ç¢ºãªå‘¨æ³¢æ•°ï¼ˆHzï¼‰ã«å¤‰æ›"""
    # A4 = 440Hz ã‚’åŸºæº–ã¨ã—ãŸ12å¹³å‡å¾‹
    note_frequencies = {
        'C': -9, 'C#': -8, 'Db': -8, 'D': -7, 'D#': -6, 'Eb': -6, 'E': -5,
        'F': -4, 'F#': -3, 'Gb': -3, 'G': -2, 'G#': -1, 'Ab': -1, 'A': 0,
        'A#': 1, 'Bb': 1, 'B': 2
    }

    try:
        # éŸ³åã¨ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ç•ªå·ã‚’åˆ†é›¢
        if '#' in note_name or 'b' in note_name:
            note = note_name[:-1]
            octave = int(note_name[-1])
        else:
            note = note_name[:-1]
            octave = int(note_name[-1])

        # A4ã‹ã‚‰ã®åŠéŸ³æ•°ã‚’è¨ˆç®—
        base_octave = 4
        semitones_from_a4 = (octave - base_octave) * 12 + note_frequencies.get(note, 0)

        # 12å¹³å‡å¾‹ã§å‘¨æ³¢æ•°ã‚’è¨ˆç®—: f = 440 * 2^(n/12)
        frequency = 440.0 * (2.0 ** (semitones_from_a4 / 12.0))

        print(f"[Musical] {note_name} â†’ {frequency:.2f} Hz")
        return frequency

    except (ValueError, KeyError):
        print(f"[Musical] Warning: Invalid note '{note_name}', using C4 (261.63 Hz)")
        return 261.63  # C4ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

def generate_musical_tone(frequency: float, duration: float, phoneme: str = "a") -> np.ndarray:
    """æŒ‡å®šã•ã‚ŒãŸå‘¨æ³¢æ•°ã¨é•·ã•ã§éŸ³æ¥½çš„ã§è‡ªç„¶ãªå¥³æ€§æ­Œå£°ã‚’ç”Ÿæˆ"""
    num_samples = int(SAMPLE_RATE * duration)
    t = np.linspace(0, duration, num_samples, endpoint=False)

    # å¥³æ€§ã‚‰ã—ã„éŸ³å£°ç‰¹æ€§ã‚’å¼·åŒ–
    # åŸºæœ¬å‘¨æ³¢æ•°ã‚’ã‚ˆã‚Šé«˜ãèª¿æ•´ï¼ˆå¥³æ€§ã‚‰ã—ã•å‘ä¸Šï¼‰
    feminine_frequency = frequency * (2 ** (FEMALE_VOICE_PITCH_BOOST / 12.0))

    # ã‚ˆã‚Šå¥³æ€§ã‚‰ã—ã„ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆï¼ˆå°‘ã—é€Ÿã‚ã§ç¹Šç´°ï¼‰
    vibrato_rate = 5.2  # Hz (å¥³æ€§çš„ãªãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆ)
    vibrato_depth = 0.08  # 8%ã®å‘¨æ³¢æ•°å¤‰å‹•ï¼ˆã‚ˆã‚Šè¡¨ç¾åŠ›è±Šã‹ï¼‰
    frequency_modulated = feminine_frequency * (1 + vibrato_depth * np.sin(2 * np.pi * vibrato_rate * t))

    # åŸºæœ¬æ³¢å½¢ï¼ˆã‚ˆã‚Šãƒªã‚¢ãƒ«ãªåˆæˆï¼‰
    fundamental = np.sin(2 * np.pi * frequency_modulated * t)

    # äººé–“ã®éŸ³å£°ã«ã‚ˆã‚Šè¿‘ã„å€éŸ³æ§‹æˆ
    phoneme_harmonics = {
        'a': [1.0, 0.7, 0.5, 0.3, 0.2, 0.1],  # ã€Œã‚ã€- æ˜ã‚‹ãé–‹æ”¾çš„
        'e': [1.0, 0.5, 0.8, 0.4, 0.2, 0.1],  # ã€Œãˆã€- é‹­ã„é«˜å‘¨æ³¢æˆåˆ†
        'i': [1.0, 0.3, 0.9, 0.5, 0.3, 0.2],  # ã€Œã„ã€- é«˜ã„å…±æŒ¯
        'o': [1.0, 0.9, 0.4, 0.6, 0.3, 0.2],  # ã€ŒãŠã€- ä¸¸ãæš–ã‹ã„
        'u': [1.0, 0.6, 0.2, 0.4, 0.5, 0.3],  # ã€Œã†ã€- ä½ã„å…±æŒ¯
    }

    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã‚’æ¨¡æ“¬ï¼ˆå¥³æ€§ã®éŸ³å£°å…±æŒ¯ç‰¹æ€§ã«æœ€é©åŒ–ï¼‰
    formant_frequencies = {
        'a': [950, 1400, 2800],    # ã‚ - å¥³æ€§ã®æ˜ã‚‹ã„å…±æŒ¯ï¼ˆé«˜ã‚ï¼‰
        'e': [600, 2100, 2900],    # ãˆ - å¥³æ€§ã®é«˜ã„å…±æŒ¯
        'i': [350, 2600, 3400],    # ã„ - å¥³æ€§ã®æœ€é«˜å…±æŒ¯
        'o': [600, 1050, 2600],    # ãŠ - å¥³æ€§ã®ä¸­ä½åŸŸå…±æŒ¯
        'u': [350, 950, 2500],     # ã† - å¥³æ€§ã®ä½åŸŸå…±æŒ¯
    }

    harmonics = phoneme_harmonics.get(phoneme, phoneme_harmonics['a'])
    formants = formant_frequencies.get(phoneme, formant_frequencies['a'])

    # è¤‡é›‘ãªæ³¢å½¢åˆæˆï¼ˆäººé–“ã®å£°é“ã‚’æ¨¡æ“¬ï¼‰
    waveform = np.zeros_like(fundamental)

    # åŸºæœ¬å€éŸ³ã®è¿½åŠ 
    for i, amplitude in enumerate(harmonics):
        harmonic_freq = frequency * (i + 1)
        if harmonic_freq < SAMPLE_RATE / 2:
            waveform += amplitude * np.sin(2 * np.pi * harmonic_freq * t)

    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆï¼ˆå…±æŒ¯ï¼‰ã®è¿½åŠ 
    for formant_freq in formants:
        if formant_freq < SAMPLE_RATE / 2:
            formant_contribution = 0.3 * np.sin(2 * np.pi * formant_freq * t)
            # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã¯æ™‚é–“ã¨å…±ã«æ¸›è¡°
            formant_envelope = np.exp(-t * 2)
            waveform += formant_contribution * formant_envelope

    # ã‚ˆã‚Šè‡ªç„¶ãªã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ï¼ˆäººé–“ã®æ­Œå£°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    attack_time = 0.05   # 50ms - é€Ÿã‚„ã‹ãªç«‹ã¡ä¸ŠãŒã‚Š
    decay_time = 0.1     # 100ms - è‡ªç„¶ãªæ¸›è¡°
    sustain_level = 0.85 # 85% - æ­Œå£°ã¯æ¯”è¼ƒçš„å®‰å®š
    release_time = 0.2   # 200ms - è‡ªç„¶ãªä½™éŸ»

    envelope = np.ones_like(t)
    attack_samples = int(attack_time * SAMPLE_RATE)
    decay_samples = int(decay_time * SAMPLE_RATE)
    release_samples = int(release_time * SAMPLE_RATE)

    # ã‚¹ãƒ ãƒ¼ã‚ºãªã‚¢ã‚¿ãƒƒã‚¯ï¼ˆæ­Œå£°çš„ãªç«‹ã¡ä¸ŠãŒã‚Šï¼‰
    if len(envelope) > attack_samples:
        envelope[:attack_samples] = np.sin(np.linspace(0, np.pi/2, attack_samples))**2

    # ãƒ‡ã‚£ã‚±ã‚¤ï¼ˆè‡ªç„¶ãªæ¸›è¡°ï¼‰
    if len(envelope) > attack_samples + decay_samples:
        envelope[attack_samples:attack_samples + decay_samples] = np.linspace(1, sustain_level, decay_samples)

    # ãƒªãƒªãƒ¼ã‚¹ï¼ˆè‡ªç„¶ãªä½™éŸ»ï¼‰
    if len(envelope) > release_samples:
        release_curve = np.sin(np.linspace(np.pi/2, 0, release_samples))**2
        envelope[-release_samples:] = sustain_level * release_curve

    # æ¯ç¶™ãåŠ¹æœï¼ˆå¾®ç´°ãªéŸ³é‡å¤‰å‹•ï¼‰
    breath_pattern = 1 + 0.02 * np.sin(2 * np.pi * 0.5 * t)  # 0.5Hzã®ç·©ã‚„ã‹ãªå¤‰å‹•
    envelope *= breath_pattern

    # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã‚’é©ç”¨
    waveform *= envelope

    # ã‚ˆã‚Šè‡ªç„¶ãªæ­£è¦åŒ–ï¼ˆæ­Œå£°ã®éŸ³é‡æ„Ÿï¼‰
    if np.max(np.abs(waveform)) > 0:
        waveform *= 0.8 / np.max(np.abs(waveform))  # å°‘ã—å¤§ãã‚ã®éŸ³é‡

    # å¾®ç´°ãªãƒã‚¤ã‚ºè¿½åŠ ï¼ˆäººé–“ã‚‰ã—ã•ï¼‰
    natural_noise = np.random.normal(0, 0.001, len(waveform))
    waveform += natural_noise

    return waveform

def apply_pitch_time_control(input_audio_path: str, output_audio_path: str,
                            target_frequencies: list, target_durations: list) -> bool:
    """Edge TTSéŸ³å£°ã«ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒ¬ãƒƒãƒã‚’é©ç”¨ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ–¹å¼ï¼‰"""
    try:
        print(f"[Note Keep Pipeline] Applying note-keeping approach to: {input_audio_path}")
        print(f"[Note Keep Pipeline] Target frequencies: {target_frequencies[:5]}")  # First 5
        print(f"[Note Keep Pipeline] Target durations: {target_durations[:5]}")      # First 5

        # WAVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with wave.open(input_audio_path, 'rb') as wav_file:
            frames = wav_file.readframes(wav_file.getnframes())
            channels = wav_file.getnchannels()
            sample_width = wav_file.getsampwidth()
            frame_rate = wav_file.getframerate()

        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ numpy é…åˆ—ã«å¤‰æ›
        if sample_width == 2:  # 16-bit
            audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
        else:
            print(f"[Note Keep Pipeline] Warning: Unsupported sample width: {sample_width}")
            return False

        # ã‚¹ãƒ†ãƒ¬ã‚ªã‹ã‚‰ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if channels == 2:
            audio_data = audio_data[::2]  # å·¦ãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿ä½¿ç”¨

        original_duration = len(audio_data) / frame_rate
        target_total_duration = sum(target_durations)

        print(f"[Note Keep Pipeline] Original duration: {original_duration:.2f}s")
        print(f"[Note Keep Pipeline] Target duration: {target_total_duration:.2f}s")

        # éŸ³ç¬¦ã‚­ãƒ¼ãƒ—æ–¹å¼: ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒ¬ãƒƒãƒã®ä»£ã‚ã‚Šã«å¿…è¦ã«å¿œã˜ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if target_total_duration > original_duration:
            # éŸ³ã‚’å»¶é•·ã™ã‚‹å ´åˆ: éŸ³å£°ã®å¾ŒåŠéƒ¨åˆ†ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦å»¶é•·
            extend_samples = int((target_total_duration - original_duration) * frame_rate)

            # éŸ³å£°ã®å¾ŒåŠ30%ã‚’ç¹°ã‚Šè¿”ã—ç”¨éŸ³æºã¨ã—ã¦ä½¿ç”¨
            loop_start = int(len(audio_data) * 0.7)
            loop_section = audio_data[loop_start:]

            # å¿…è¦ãªåˆ†ã ã‘ç¹°ã‚Šè¿”ã—ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            extended_audio = []
            remaining_samples = extend_samples
            while remaining_samples > 0:
                add_samples = min(remaining_samples, len(loop_section))
                extended_audio.extend(loop_section[:add_samples])
                remaining_samples -= add_samples

            # å…ƒã®éŸ³å£°ã¨å»¶é•·éƒ¨åˆ†ã‚’çµåˆ
            audio_data = np.concatenate([audio_data, np.array(extended_audio)])
            print(f"[Note Keep Pipeline] Extended audio by {extend_samples} samples using note-keeping")
        elif target_total_duration < original_duration:
            # éŸ³ã‚’çŸ­ç¸®ã™ã‚‹å ´åˆ: å¿…è¦ãªé•·ã•ã¾ã§ãƒˆãƒªãƒŸãƒ³ã‚°
            target_samples = int(target_total_duration * frame_rate)
            audio_data = audio_data[:target_samples]
            print(f"[Note Keep Pipeline] Trimmed audio to {target_samples} samples")

        # å€‹åˆ¥éŸ³ç¬¦ãƒ”ãƒƒãƒåˆ¶å¾¡ï¼ˆå„éŸ³ç¬¦ã”ã¨ã«ç²¾å¯†ãªåˆ¶å¾¡ï¼‰
        if target_frequencies and len(target_frequencies) == len(target_durations):
            print(f"[Note Keep Pipeline] Applying individual note pitch control for {len(target_frequencies)} notes")

            # å„éŸ³ç¬¦ã®é–‹å§‹æ™‚åˆ»ã‚’è¨ˆç®—
            note_start_times = []
            current_time = 0
            for duration in target_durations:
                note_start_times.append(current_time)
                current_time += duration

            # åŸºæº–å‘¨æ³¢æ•°ï¼ˆC4 = 261.63 Hzï¼‰
            base_frequency = 261.63

            # å„éŸ³ç¬¦ã«å¯¾ã—ã¦ãƒ”ãƒƒãƒèª¿æ•´ã‚’é©ç”¨
            modified_audio = np.copy(audio_data)

            for i, (target_freq, duration, start_time) in enumerate(zip(target_frequencies, target_durations, note_start_times)):
                # è©²å½“ã™ã‚‹éŸ³ç¬¦ã®æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
                start_sample = int(start_time * frame_rate)
                end_sample = int((start_time + duration) * frame_rate)

                # ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if start_sample >= len(audio_data) or end_sample > len(audio_data):
                    break

                # è©²å½“ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
                segment = audio_data[start_sample:end_sample]
                if len(segment) == 0:
                    continue

                # éŸ³ç¬¦ç‰¹æœ‰ã®ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆæ¯”ç‡ã‚’è¨ˆç®—
                pitch_shift_ratio = target_freq / base_frequency

                if abs(pitch_shift_ratio - 1.0) > 0.05:  # 5%ä»¥ä¸Šã®å¤‰æ›´æ™‚ã®ã¿é©ç”¨
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”¨FFTãƒ™ãƒ¼ã‚¹ã®ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆ
                    segment_fft = np.fft.fft(segment)
                    segment_freqs = np.fft.fftfreq(len(segment), 1/frame_rate)

                    # ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆã‚’é©ç”¨
                    shifted_segment_fft = np.zeros_like(segment_fft)
                    for j, freq in enumerate(segment_freqs):
                        new_freq_idx = int(j * pitch_shift_ratio)
                        if 0 <= new_freq_idx < len(shifted_segment_fft):
                            shifted_segment_fft[new_freq_idx] = segment_fft[j]

                    # ä¿®æ­£ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æˆ»ã™
                    modified_segment = np.real(np.fft.ifft(shifted_segment_fft))
                    modified_audio[start_sample:end_sample] = modified_segment

                    print(f"[Note Keep Pipeline] Note {i+1}: {target_freq:.1f} Hz (shift: {pitch_shift_ratio:.2f}x)")

            audio_data = modified_audio
            print(f"[Note Keep Pipeline] Applied individual pitch control to all {len(target_frequencies)} notes")

        # å¥³æ€§ã‚‰ã—ã•ã®å‘ä¸Šï¼ˆãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
        feminization_boost = 1.2  # 20%é«˜ãï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
        fft_data = np.fft.fft(audio_data)
        freqs = np.fft.fftfreq(len(audio_data), 1/frame_rate)

        shifted_fft = np.zeros_like(fft_data)
        for i, freq in enumerate(freqs):
            new_freq_idx = int(i * feminization_boost)
            if 0 <= new_freq_idx < len(shifted_fft):
                shifted_fft[new_freq_idx] = fft_data[i]

        audio_data = np.real(np.fft.ifft(shifted_fft))
        print(f"[Note Keep Pipeline] Applied balanced feminization boost: {feminization_boost}x")

        # æ­Œå£°æ€§å‘ä¸Š: ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆåŠ¹æœã¨éŸ³ç¬¦è¡¨ç¾åŠ›ã®è¿½åŠ 
        if target_frequencies and len(target_frequencies) == len(target_durations):
            print(f"[Note Keep Pipeline] Adding vibrato and vocal expression")

            # å„éŸ³ç¬¦ã®é–‹å§‹æ™‚åˆ»ã‚’å†è¨ˆç®—ï¼ˆæ”¹è‰¯å¾Œã®éŸ³å£°ç”¨ï¼‰
            note_start_times = []
            current_time = 0
            for duration in target_durations:
                note_start_times.append(current_time)
                current_time += duration

            # ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆåŠ¹æœã‚’é©ç”¨
            for i, (target_freq, duration, start_time) in enumerate(zip(target_frequencies, target_durations, note_start_times)):
                # è©²å½“ã™ã‚‹éŸ³ç¬¦ã®æ™‚é–“ç¯„å›²ã‚’è¨ˆç®—
                start_sample = int(start_time * frame_rate)
                end_sample = int((start_time + duration) * frame_rate)

                # ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if start_sample >= len(audio_data) or end_sample > len(audio_data):
                    break

                # é•·ã„éŸ³ç¬¦ï¼ˆ1.0ç§’ä»¥ä¸Šï¼‰ã«ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆåŠ¹æœã‚’é©ç”¨
                if duration >= 1.0:
                    segment = audio_data[start_sample:end_sample]
                    if len(segment) > 0:
                        # ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆå‘¨æ³¢æ•°ï¼ˆ4-6 Hzï¼‰ã¨ãƒ‡ãƒ—ã‚¹ï¼ˆÂ±2%ï¼‰
                        vibrato_freq = 5.0  # Hz
                        vibrato_depth = 0.02  # Â±2%

                        # æ™‚é–“è»¸ã‚’ç”Ÿæˆ
                        t = np.linspace(0, duration, len(segment))

                        # ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆå¤‰èª¿ã‚’ç”Ÿæˆ
                        vibrato_modulation = 1.0 + vibrato_depth * np.sin(2 * np.pi * vibrato_freq * t)

                        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒ“ãƒ–ãƒ©ãƒ¼ãƒˆåŠ¹æœã‚’é©ç”¨
                        audio_data[start_sample:end_sample] = segment * vibrato_modulation

                        print(f"[Note Keep Pipeline] Applied vibrato to note {i+1} (duration: {duration:.1f}s)")

            print(f"[Note Keep Pipeline] Enhanced vocal expression completed")

        # éŸ³é‡æ­£è¦åŒ–
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            audio_data = audio_data / max_val * 0.8  # 80%ã®æœ€å¤§éŸ³é‡

        # 16-bit PCM ã«å¤‰æ›ã—ã¦ä¿å­˜
        audio_16bit = (audio_data * 32767).astype(np.int16)

        with wave.open(output_audio_path, 'wb') as output_wav:
            output_wav.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
            output_wav.setsampwidth(2)  # 16-bit
            output_wav.setframerate(frame_rate)
            output_wav.writeframes(audio_16bit.tobytes())

        print(f"[Note Keep Pipeline] Successfully processed and saved: {output_audio_path}")
        final_duration = len(audio_data) / frame_rate
        print(f"[Note Keep Pipeline] Final duration: {final_duration:.2f}s (target: {target_total_duration:.2f}s)")
        return True

    except Exception as e:
        print(f"[Note Keep Pipeline] Error in note-keep processing: {e}")
        return False

def create_musical_vocals(lyrics: str, notes_list: list, durations_list: list, output_path: str) -> bool:
    """æ­Œè©ã¨ãƒãƒ¼ãƒˆæƒ…å ±ã‹ã‚‰éŸ³æ¥½çš„ãªæ­Œå£°ã‚’ç”Ÿæˆ"""
    print(f"[Musical] Creating vocals for '{lyrics}' with {len(notes_list)} notes")

    # æ­Œè©ã‚’æ–‡å­—å˜ä½ã§åˆ†å‰²
    lyrics_chars = list(lyrics)

    # ãƒãƒ¼ãƒˆæ•°ã¨æ­Œè©æ–‡å­—æ•°ã‚’èª¿æ•´
    if len(lyrics_chars) != len(notes_list):
        print(f"[Musical] Adjusting lyrics length: {len(lyrics_chars)} chars to {len(notes_list)} notes")
        if len(lyrics_chars) < len(notes_list):
            # æ­Œè©ãŒå°‘ãªã„å ´åˆã¯æœ€å¾Œã®æ–‡å­—ã‚’å»¶é•·
            while len(lyrics_chars) < len(notes_list):
                lyrics_chars.append(lyrics_chars[-1] if lyrics_chars else 'ã‚')
        else:
            # æ­Œè©ãŒå¤šã„å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            lyrics_chars = lyrics_chars[:len(notes_list)]

    # æ—¥æœ¬èªéŸ³ç´ ãƒãƒƒãƒ”ãƒ³ã‚°
    phoneme_map = {
        'ã‹': 'a', 'ã': 'i', 'ã': 'u', 'ã‘': 'e', 'ã“': 'o',
        'ãŒ': 'a', 'ã': 'i', 'ã': 'u', 'ã’': 'e', 'ã”': 'o',
        'ã•': 'a', 'ã—': 'i', 'ã™': 'u', 'ã›': 'e', 'ã': 'o',
        'ã–': 'a', 'ã˜': 'i', 'ãš': 'u', 'ãœ': 'e', 'ã': 'o',
        'ãŸ': 'a', 'ã¡': 'i', 'ã¤': 'u', 'ã¦': 'e', 'ã¨': 'o',
        'ã ': 'a', 'ã¢': 'i', 'ã¥': 'u', 'ã§': 'e', 'ã©': 'o',
        'ãª': 'a', 'ã«': 'i', 'ã¬': 'u', 'ã­': 'e', 'ã®': 'o',
        'ã¯': 'a', 'ã²': 'i', 'ãµ': 'u', 'ã¸': 'e', 'ã»': 'o',
        'ã°': 'a', 'ã³': 'i', 'ã¶': 'u', 'ã¹': 'e', 'ã¼': 'o',
        'ã±': 'a', 'ã´': 'i', 'ã·': 'u', 'ãº': 'e', 'ã½': 'o',
        'ã¾': 'a', 'ã¿': 'i', 'ã‚€': 'u', 'ã‚': 'e', 'ã‚‚': 'o',
        'ã‚„': 'a', 'ã‚†': 'u', 'ã‚ˆ': 'o',
        'ã‚‰': 'a', 'ã‚Š': 'i', 'ã‚‹': 'u', 'ã‚Œ': 'e', 'ã‚': 'o',
        'ã‚': 'a', 'ã‚': 'i', 'ã‚‘': 'e', 'ã‚’': 'o', 'ã‚“': 'u',
        'ã‚': 'a', 'ã„': 'i', 'ã†': 'u', 'ãˆ': 'e', 'ãŠ': 'o'
    }

    # éŸ³æ¥½çš„æ³¢å½¢ã‚’ç”Ÿæˆ
    vocal_segments = []

    for char, note, duration in zip(lyrics_chars, notes_list, durations_list):
        # å‘¨æ³¢æ•°ã‚’å–å¾—
        frequency = midi_note_to_frequency(note)

        # éŸ³ç´ ã‚’å–å¾—
        phoneme = phoneme_map.get(char, 'a')

        # éŸ³æ¥½çš„ãªé•·ã•ã«èª¿æ•´ï¼ˆæœ€ä½1ç§’ï¼‰
        musical_duration = max(MUSICAL_NOTE_DURATION, float(duration))

        # éŸ³æ¥½çš„éŸ³ã‚’ç”Ÿæˆ
        segment = generate_musical_tone(frequency, musical_duration, phoneme)
        vocal_segments.append(segment)

        print(f"[Musical] {char} ({phoneme}) â†’ {frequency:.2f} Hz Ã— {musical_duration:.1f}s")

    # å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
    full_vocal = np.concatenate(vocal_segments)

    print(f"[Musical] Generated {len(full_vocal)} samples ({len(full_vocal)/SAMPLE_RATE:.2f} seconds)")

    # WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    return save_musical_audio(full_vocal, output_path)

def save_musical_audio(waveform: np.ndarray, output_path: str) -> bool:
    """éŸ³æ¥½çš„æ³¢å½¢ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    try:
        # 16-bit PCMå½¢å¼ã§ä¿å­˜
        waveform_16bit = (waveform * 32767).astype(np.int16)

        with wave.open(output_path, 'w') as wav_file:
            wav_file.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(SAMPLE_RATE)
            wav_file.writeframes(waveform_16bit.tobytes())

        print(f"[Musical] Saved audio: {output_path}")
        return True

    except Exception as e:
        print(f"[Musical] Error saving audio: {e}")
        return False

async def generate_edge_tts_audio_with_pitch(text: str, voice: str, output_path: str, notes_list: list, durations_list: list) -> bool:
    """Edge TTSã‚’ä½¿ç”¨ã—ã¦éŸ³ç¨‹åˆ¶å¾¡ä»˜ãéŸ³å£°ç”Ÿæˆ"""
    try:
        # Edge TTS voiceè¨­å®š
        voice_map = {
            "edge_tts_nanami": "ja-JP-NanamiNeural",
            "edge_tts_keita": "ja-JP-KeitaNeural"
        }

        edge_voice = voice_map.get(voice, "ja-JP-NanamiNeural")

        print(f"[Edge TTS] Generating musical speech with voice: {edge_voice}")
        print(f"[Edge TTS] Text: {text}")
        print(f"[Edge TTS] Notes: {notes_list}")

        # ãƒ”ãƒƒãƒã¨é€Ÿåº¦ã‚’ç›´æ¥è¨ˆç®—ï¼ˆå¼•æ•°ãƒ™ãƒ¼ã‚¹TTSåˆ¶å¾¡ï¼‰
        if not notes_list or len(notes_list) == 0:
            pitch_percentage = 0
            rate = None
        else:
            # æœ€åˆã®ãƒãƒ¼ãƒˆã®éŸ³ç¨‹ã‚’ä½¿ç”¨ï¼ˆå˜ä¸€ãƒ”ãƒƒãƒåˆ¶å¾¡ï¼‰
            first_note = notes_list[0]

            # MIDIéŸ³åã‚’A4ã‹ã‚‰ã®åŠéŸ³æ•°ã«å¤‰æ›
            note_frequencies = {
                'C': -9, 'C#': -8, 'Db': -8, 'D': -7, 'D#': -6, 'Eb': -6, 'E': -5,
                'F': -4, 'F#': -3, 'Gb': -3, 'G': -2, 'G#': -1, 'Ab': -1, 'A': 0,
                'A#': 1, 'Bb': 1, 'B': 2
            }

            try:
                # éŸ³åã¨ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ç•ªå·ã‚’åˆ†é›¢
                if '#' in first_note or 'b' in first_note:
                    note = first_note[:-1]
                    octave = int(first_note[-1])
                else:
                    note = first_note[:-1]
                    octave = int(first_note[-1])

                # A4ã‹ã‚‰ã®åŠéŸ³æ•°ã‚’è¨ˆç®—
                base_octave = 4
                semitones_from_a4 = (octave - base_octave) * 12 + note_frequencies.get(note, 0)
            except (ValueError, KeyError):
                print(f"[Argument TTS] Warning: Invalid note '{first_note}', using default")
                semitones_from_a4 = 0  # A4ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            # ã‚ˆã‚Šé©åˆ‡ãªãƒ”ãƒƒãƒç¯„å›²ï¼ˆå¥³æ€§ã®å£°ç”¨ã«èª¿æ•´ï¼‰+ éŸ³éšã‚’æ˜ç¢ºã«ã™ã‚‹
            base_pitch_multiplier = 10  # ã‚ˆã‚Šæ˜ç¢ºãªéŸ³éšå¤‰åŒ–
            feminine_pitch_offset = FEMALE_VOICE_PITCH_BOOST * base_pitch_multiplier  # å¥³æ€§ã‚‰ã—ã„é«˜éŸ³
            pitch_percentage = max(-50, min(80, semitones_from_a4 * base_pitch_multiplier + feminine_pitch_offset))

            # ç·æ™‚é–“ã«åŸºã¥ãèª­ã¿ä¸Šã’é€Ÿåº¦ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ã«èª¿æ•´ï¼‰
            total_duration = sum(durations_list)
            if total_duration < 3:
                rate = "+20%"  # é€Ÿã‚
            elif total_duration > 15:
                rate = "-50%"  # éå¸¸ã«é…ã‚ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ï¼‰
            elif total_duration > 8:
                rate = "-40%"  # é…ã‚ï¼ˆæ”¹å–„ç‰ˆï¼‰
            else:
                rate = "-20%"  # ã‚„ã‚„é…ã‚ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’èª¿æ•´ï¼‰

        # ãƒ”ãƒƒãƒã‚’Hzå½¢å¼ã«å¤‰æ›ï¼ˆEdge TTSè¦æ±‚ä»•æ§˜ï¼‰
        pitch_hz = int(pitch_percentage * 2)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’Hzã«å¤‰æ›
        pitch_value = f"{pitch_hz:+d}Hz"
        print(f"[Argument-based TTS] Using Edge TTS argument control: pitch={pitch_value}, rate={rate}")

        # Communicateã®å¼•æ•°ã‚’å‹•çš„ã«æ§‹ç¯‰
        communicate_args = {
            'text': text,
            'voice': edge_voice,
            'pitch': pitch_value
        }
        if rate is not None:
            communicate_args['rate'] = rate

        communicate = edge_tts.Communicate(**communicate_args)

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_path = output_path + "_temp.mp3"
        await communicate.save(temp_path)

        # MP3ã‹ã‚‰WAVã«å¤‰æ›ï¼ˆé«˜å“è³ªå¤‰æ›ï¼‰
        conversion_success = convert_mp3_to_wav(temp_path, output_path)
        if not conversion_success:
            print(f"[Edge TTS] Warning: MP3 to WAV conversion failed for {output_path}")
            return False

        print(f"[Edge TTS] Musical audio generated successfully: {output_path}")
        return True

    except Exception as e:
        print(f"[Edge TTS] Error generating musical audio: {e}")
        return False

async def generate_edge_tts_audio(text: str, voice: str, output_path: str) -> bool:
    """Edge TTSã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ç”Ÿæˆï¼ˆæ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€äº’æ›æ€§ç¶­æŒï¼‰"""
    try:
        # Edge TTS voiceè¨­å®š
        voice_map = {
            "edge_tts_nanami": "ja-JP-NanamiNeural",
            "edge_tts_keita": "ja-JP-KeitaNeural"
        }

        edge_voice = voice_map.get(voice, "ja-JP-NanamiNeural")

        print(f"[Edge TTS] Generating speech with voice: {edge_voice}")
        print(f"[Edge TTS] Text: {text}")

        # Edge TTSã§éŸ³å£°ç”Ÿæˆ
        communicate = edge_tts.Communicate(text, edge_voice)

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_path = output_path + "_temp.mp3"
        await communicate.save(temp_path)

        # MP3ã‹ã‚‰WAVã«å¤‰æ›ï¼ˆé«˜å“è³ªå¤‰æ›ï¼‰
        conversion_success = convert_mp3_to_wav(temp_path, output_path)
        if not conversion_success:
            print(f"[Edge TTS] Warning: MP3 to WAV conversion failed for {output_path}")
            return False

        print(f"[Edge TTS] Audio generated successfully: {output_path}")
        return True

    except Exception as e:
        print(f"[Edge TTS] Error generating audio: {e}")
        return False

def create_mathematical_audio_fallback(lyrics: str, notes_list: list, durations_list: list, output_path: str, total_duration: float):
    """Edge TTSåˆ©ç”¨ä¸å¯æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ•°å­¦çš„åˆæˆï¼‰"""
    print("[Fallback] Using mathematical synthesis (Edge TTS not available)")

    # ç°¡ç•¥åŒ–ã•ã‚ŒãŸæ­£å¼¦æ³¢ãƒ™ãƒ¼ã‚¹ã®éŸ³å£°ç”Ÿæˆ
    import math

    with open(output_path, 'wb') as f:
        # éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        sample_rate = 44100
        channels = 1  # ãƒ¢ãƒãƒ©ãƒ«
        bits_per_sample = 16
        audio_samples = int(sample_rate * total_duration)
        data_size = audio_samples * channels * (bits_per_sample // 8)
        file_size = 36 + data_size

        # RIFFãƒ˜ãƒƒãƒ€ãƒ¼
        f.write(b'RIFF')
        f.write(file_size.to_bytes(4, 'little'))
        f.write(b'WAVE')

        # fmtãƒãƒ£ãƒ³ã‚¯
        f.write(b'fmt ')
        f.write((16).to_bytes(4, 'little'))
        f.write((1).to_bytes(2, 'little'))
        f.write(channels.to_bytes(2, 'little'))
        f.write(sample_rate.to_bytes(4, 'little'))
        f.write((sample_rate * channels * bits_per_sample // 8).to_bytes(4, 'little'))
        f.write((channels * bits_per_sample // 8).to_bytes(2, 'little'))
        f.write(bits_per_sample.to_bytes(2, 'little'))

        # dataãƒãƒ£ãƒ³ã‚¯
        f.write(b'data')
        f.write(data_size.to_bytes(4, 'little'))

        # ç°¡å˜ãªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        def note_to_freq(note_name):
            if len(note_name) < 2:
                return 440.0
            note = note_name[:-1]
            octave = int(note_name[-1])
            note_offsets = {'C': 0, 'D': 2, 'E': 4, 'F': 5, 'G': 7, 'A': 9, 'B': 11}
            if note not in note_offsets:
                return 440.0
            midi_number = (octave - 4) * 12 + note_offsets[note] - 9
            return 440.0 * (2 ** (midi_number / 12))

        audio_data = bytearray()
        current_time = 0.0

        for note_name, duration in zip(notes_list, durations_list):
            frequency = note_to_freq(note_name)
            note_samples = int(sample_rate * duration)

            for sample_idx in range(note_samples):
                time = sample_idx / sample_rate

                # ã‚·ãƒ³ãƒ—ãƒ«ãªæ­£å¼¦æ³¢
                sample_value = 0.3 * math.sin(2 * math.pi * frequency * time)

                # ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—
                if time < 0.05:
                    envelope = time / 0.05
                elif time > duration - 0.05:
                    envelope = (duration - time) / 0.05
                else:
                    envelope = 1.0

                sample_value *= envelope

                # 16bitã‚µãƒ³ãƒ—ãƒ«ã«å¤‰æ›
                sample_16bit = int(sample_value * 16000)
                sample_16bit = max(-32768, min(32767, sample_16bit))

                audio_data.extend(sample_16bit.to_bytes(2, 'little', signed=True))

            current_time += duration

        f.write(audio_data)

@app.post("/api/synthesize", response_model=SynthesisResponse)
async def synthesize(request: SynthesisRequest):
    """éŸ³å£°åˆæˆï¼ˆEdge TTSçµ±åˆç‰ˆï¼‰"""
    try:
        print(f"[Enhanced Mock] Synthesis request received:")
        print(f"  Lyrics: {request.lyrics}")
        print(f"  Notes: {request.notes}")
        print(f"  Durations: {request.durations}")
        print(f"  Current model: {current_model}")

        # å…¥åŠ›æ¤œè¨¼
        notes_list = [note.strip() for note in request.notes.split('|')]
        durations_list = [dur.strip() for dur in request.durations.split('|')]

        if len(notes_list) != len(durations_list):
            raise HTTPException(
                status_code=400,
                detail=f"Notes count ({len(notes_list)}) must match durations count ({len(durations_list)})"
            )

        # ç·å†ç”Ÿæ™‚é–“è¨ˆç®—
        total_duration = sum(float(dur) for dur in durations_list)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path = Path(request.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ­Œè©ã‚’æ—¥æœ¬èªç™ºéŸ³ã«æœ€é©åŒ–
        japanese_lyrics = await convert_lyrics_to_japanese_phonetics(request.lyrics)
        print(f"[Enhanced Mock] Converted lyrics: '{request.lyrics}' -> '{japanese_lyrics}'")

        synthesis_success = False
        engine_info = "Unknown"

        print(f"[Enhanced Mock] Synthesis mode: {SYNTHESIS_MODE}")

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ–¹å¼ï¼ˆedge_tts_post ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if SYNTHESIS_MODE == "edge_tts_post" and EDGE_TTS_AVAILABLE and current_model.startswith("edge_tts_"):
            print(f"[Enhanced Mock] Using Sequential Pipeline: Edge TTS + Post-processing")
            try:
                # Step 1: Edge TTSã§åŸºæœ¬éŸ³å£°ç”Ÿæˆ
                temp_edge_output = str(output_path).replace(".wav", "_temp_edge.wav")
                durations_float = [float(dur) for dur in durations_list]
                edge_success = await generate_edge_tts_with_fixed_ssml(
                    japanese_lyrics,
                    current_model,
                    temp_edge_output,
                    notes_list,
                    durations_float
                )

                if edge_success and Path(temp_edge_output).exists():
                    print(f"[Enhanced Mock] Edge TTS step completed, applying post-processing...")

                    # Step 2: éŸ³éšã¨éŸ³é•·ã®å¾Œå‡¦ç†ã‚’é©ç”¨
                    target_frequencies = [midi_note_to_frequency(note) for note in notes_list]
                    post_success = apply_pitch_time_control(
                        temp_edge_output,
                        str(output_path),
                        target_frequencies,
                        durations_float
                    )

                    if post_success:
                        synthesis_success = True
                        engine_info = f"Sequential Pipeline (Edge TTS + Post-processing)"
                        print(f"[Enhanced Mock] Sequential pipeline completed successfully")

                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    try:
                        Path(temp_edge_output).unlink()
                    except:
                        pass
                else:
                    print(f"[Enhanced Mock] Edge TTS step failed in sequential pipeline")

            except Exception as e:
                print(f"[Enhanced Mock] Sequential pipeline error: {e}")

        # Edge TTS ã®ã¿ãƒ¢ãƒ¼ãƒ‰ï¼ˆedge_tts_onlyï¼‰
        elif SYNTHESIS_MODE == "edge_tts_only" and EDGE_TTS_AVAILABLE and current_model.startswith("edge_tts_"):
            print(f"[Enhanced Mock] Using Edge TTS only mode")
            try:
                durations_float = [float(dur) for dur in durations_list]
                success = await generate_edge_tts_with_fixed_ssml(
                    japanese_lyrics,
                    current_model,
                    str(output_path),
                    notes_list,
                    durations_float
                )

                if success:
                    synthesis_success = True
                    engine_info = f"Edge TTS Only ({current_model})"
                    print(f"[Enhanced Mock] Edge TTS only mode completed successfully")

            except Exception as e:
                print(f"[Enhanced Mock] Edge TTS only mode error: {e}")

        # å¾“æ¥ã®å„ªå…ˆé †ä½ã‚·ã‚¹ãƒ†ãƒ ï¼ˆmusical_first ã¾ãŸã¯ä»–ã®ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if not synthesis_success:
            # å„ªå…ˆé †ä½1: æ–°ã—ã„éŸ³æ¥½çš„åˆæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ­£ç¢ºãªéŸ³éšãƒ»éŸ³é•·åˆ¶å¾¡ï¼‰
            print(f"[Enhanced Mock] Using Musical Synthesis System for accurate pitch and duration control")
            try:
                durations_float = [float(dur) for dur in durations_list]
                success = create_musical_vocals(
                    japanese_lyrics,
                    notes_list,
                    durations_float,
                    str(output_path)
                )

                if success:
                    synthesis_success = True
                    engine_info = "Musical Synthesis (Accurate Pitch & Duration)"
                    print(f"[Enhanced Mock] Musical synthesis completed successfully")
                else:
                    print(f"[Enhanced Mock] Musical synthesis failed, trying Edge TTS fallback")

            except Exception as e:
                print(f"[Enhanced Mock] Musical synthesis error: {e}")

            # å„ªå…ˆé †ä½2: Edge TTSï¼ˆéŸ³æ¥½çš„åˆæˆå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not synthesis_success and EDGE_TTS_AVAILABLE and current_model.startswith("edge_tts_"):
                print(f"[Enhanced Mock] Fallback to Edge TTS: {current_model}")
                try:
                    durations_float = [float(dur) for dur in durations_list]
                    success = await generate_edge_tts_with_fixed_ssml(
                        japanese_lyrics,
                        current_model,
                        str(output_path),
                        notes_list,
                        durations_float
                    )

                    if success:
                        synthesis_success = True
                        engine_info = f"Edge TTS Fallback ({current_model})"
                        print(f"[Enhanced Mock] Edge TTS fallback completed successfully")
                    else:
                        print(f"[Enhanced Mock] Edge TTS fallback failed")

                except Exception as e:
                    print(f"[Enhanced Mock] Edge TTS fallback error: {e}")

        # å„ªå…ˆé †ä½3: ãƒ¬ã‚¬ã‚·ãƒ¼éŸ³æ¥½çš„åˆæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not synthesis_success:
            print(f"[Enhanced Mock] Using improved musical synthesis system as fallback...")
            try:
                # æ–°ã—ã„å‘¨æ³¢æ•°ãƒ™ãƒ¼ã‚¹éŸ³æ¥½åˆæˆ
                durations_float = [float(dur) for dur in durations_list]
                success = create_musical_vocals(
                    japanese_lyrics,
                    notes_list,
                    durations_float,
                    str(output_path)
                )

                if success:
                    synthesis_success = True
                    engine_info = "Musical Synthesis (Frequency-based)"
                    print(f"[Enhanced Mock] Musical synthesis completed successfully")
                else:
                    print(f"[Enhanced Mock] Musical synthesis failed, using final fallback")

            except Exception as e:
                print(f"[Enhanced Mock] Musical synthesis error: {e}")

        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ•°å­¦çš„åˆæˆ
        if not synthesis_success:
            create_mathematical_audio_fallback(
                japanese_lyrics,
                notes_list,
                durations_list,
                str(output_path),
                total_duration
            )
            engine_info = "Mathematical Synthesis (Final Fallback)"
            print(f"[Enhanced Mock] Mathematical synthesis completed")

        print(f"[Enhanced Mock] Output file: {output_path}")

        return SynthesisResponse(
            status="success",
            message=f"Enhanced synthesis completed using {engine_info} for '{japanese_lyrics}' ({len(notes_list)} notes)",
            audio_path=str(output_path),
            duration=total_duration
        )

    except ValueError as e:
        print(f"[Enhanced Mock] Input error: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Invalid input: {str(e)}"
        )
    except Exception as e:
        print(f"[Enhanced Mock] Synthesis error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Synthesis failed: {str(e)}"
        )

@app.get("/api/synthesize/{request_id}")
async def get_synthesis_status(request_id: str):
    """åˆæˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ï¼ˆå°†æ¥ã®éåŒæœŸå‡¦ç†ç”¨ï¼‰"""
    return {
        "request_id": request_id,
        "status": "completed",
        "progress": 100,
        "message": "Enhanced musical synthesis completed",
        "engine": "Musical Synthesis"
    }

@app.get("/api/generated/{filename}")
@app.head("/api/generated/{filename}")
async def serve_generated_audio(filename: str):
    """ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ä¿¡"""
    output_path = Path("outputs") / filename

    if not output_path.exists():
        raise HTTPException(
            status_code=404,
            detail=f"Audio file {filename} not found"
        )

    print(f"[Enhanced Mock] Serving audio file: {output_path}")

    # Edge TTSã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€MP3ã¨ã—ã¦é…ä¿¡
    if EDGE_TTS_AVAILABLE and current_model.startswith("edge_tts_"):
        media_type = "audio/mpeg"
    else:
        media_type = "audio/wav"

    return FileResponse(
        path=str(output_path),
        media_type=media_type,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Cache-Control": "no-cache"
        }
    )

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•è¨­å®š
if __name__ == "__main__":
    print("Starting Enhanced Mock DiffSinger Server with Edge TTS...")
    uvicorn.run(app, host="127.0.0.1", port=8001)