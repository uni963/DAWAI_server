# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ãƒ»åˆæˆãƒ«ãƒ¼ãƒˆè©³ç´°

**æœ€çµ‚æ›´æ–°**: 2025-10-05
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0

---

## ğŸ“‹ ç›®æ¬¡

1. [å®Œå…¨åˆæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#å®Œå…¨åˆæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
2. [ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥è©³ç´°](#ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥è©³ç´°)
3. [è¨€èªåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼](#è¨€èªåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼)
4. [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµŒè·¯](#ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµŒè·¯)
5. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)

---

## å®Œå…¨åˆæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### å…¨ä½“ãƒ•ãƒ­ãƒ¼å›³

```mermaid
graph TD
    A[å…¥åŠ›: Text + Notes + Durations] --> B{Language Detection}
    B -->|zh_CN| C1[Chinese Text Processor]
    B -->|ja_JP| C2[Japanese Text Processor]

    C1 --> D1[Pinyin Conversion]
    C2 --> D2[Kana/Romaji Conversion]

    D1 --> E[Phoneme Sequence]
    D2 --> E

    E --> F[MIDI Processing]
    F --> G[Acoustic Model Input Preparation]

    G --> H[GaussianDiffusion]
    H --> I[Mel-Spectrogram]

    I --> J{Pitch Extractor Enabled?}
    J -->|Yes| K[Neural F0 Prediction]
    J -->|No| L[Use MIDI F0]

    K --> M[F0 Contour]
    L --> M

    M --> N[HiFi-GAN Vocoder]
    I --> N

    N --> O[Audio Waveform]
    O --> P[Normalization]
    P --> Q[WAV Encoding]
    Q --> R[Output: WAV File]

    style A fill:#4CAF50,color:#fff
    style R fill:#2196F3,color:#fff
    style H fill:#FFC107,color:#000
    style N fill:#FFC107,color:#000
```

### ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³

```mermaid
sequenceDiagram
    participant User as Frontend
    participant API as FastAPI
    participant Router as Language Router
    participant Processor as Text Processor
    participant Engine as Synthesis Engine
    participant Acoustic as Acoustic Model
    participant PE as Pitch Extractor
    participant Vocoder as HiFi-GAN

    User->>API: POST /synthesize<br/>{text, lang, notes, durations}
    API->>API: Validate Request (Pydantic)
    API->>Router: Route by language

    Router->>Processor: Load language plugin
    Processor->>Processor: Text â†’ Phonemes
    Processor-->>Engine: Phoneme sequence

    Engine->>Engine: Parse MIDI (notes + durations)
    Engine->>Acoustic: Forward(phonemes, midi)

    Acoustic->>Acoustic: Diffusion sampling (100 steps)
    Acoustic-->>Engine: Mel-spectrogram

    alt Pitch Extractor Enabled
        Engine->>PE: Extract F0(mel)
        PE-->>Engine: F0 contour
    else Use MIDI F0
        Engine->>Engine: MIDI â†’ F0
    end

    Engine->>Vocoder: Generate(mel, f0)
    Vocoder->>Vocoder: Neural waveform generation
    Vocoder-->>Engine: Audio waveform

    Engine->>Engine: Normalize + Encode WAV
    Engine-->>API: WAV bytes
    API-->>User: Return audio file
```

---

## ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥è©³ç´°

### Phase 1: å…¥åŠ›å‡¦ç†ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

**å…¥åŠ›å½¢å¼**:
```python
class SynthesisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=1000)
    language: str = Field(default="zh_CN", regex="^[a-z]{2}_[A-Z]{2}$")
    notes: str = Field(..., description="C4 | D4 | E4")
    durations: str = Field(..., description="0.5 | 0.5 | 1.0")
    speaker_id: int = Field(default=0, ge=0)
```

**ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒã‚¯**:
```python
def validate_input(request: SynthesisRequest):
    """
    å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼

    æ¤œè¨¼é …ç›®:
    1. text: éç©ºæ–‡å­—åˆ—ã€æœ€å¤§1000æ–‡å­—
    2. language: ISO 639-1_ISO 3166-1å½¢å¼ï¼ˆä¾‹: zh_CN, ja_JPï¼‰
    3. notes: |åŒºåˆ‡ã‚Šã®MIDIéŸ³å
    4. durations: |åŒºåˆ‡ã‚Šã®ç§’æ•°
    5. notesæ•° == durationsæ•°
    """
    note_list = [n.strip() for n in request.notes.split('|') if n.strip()]
    dur_list = [d.strip() for d in request.durations.split('|') if d.strip()]

    if len(note_list) != len(dur_list):
        raise ValueError(
            f"Notes count ({len(note_list)}) != "
            f"Durations count ({len(dur_list)})"
        )

    # MIDIéŸ³åæ¤œè¨¼ï¼ˆC0-C8, #/bå¯¾å¿œï¼‰
    midi_pattern = re.compile(r'^[A-G](#|b)?[0-8]$')
    for note in note_list:
        if not midi_pattern.match(note):
            raise ValueError(f"Invalid MIDI note: {note}")

    return True
```

### Phase 2: éŸ³ç´ å¤‰æ›

#### ä¸­å›½èªãƒ‘ã‚¹

```
å…¥åŠ›: "å°é…’çªé•¿ç«æ¯›"
    â†“
jiebaåˆ†ã‹ã¡æ›¸ã: ["å°", "é…’çª", "é•¿", "ç«æ¯›"]
    â†“
G2pMå¤‰æ›: ["xiao3", "jiu3", "wo1", "chang2", "jie2", "mao2"]
    â†“
éŸ³ç´ åˆ†å‰²: ["x", "iao", "j", "iu", "w", "o", "ch", "ang", "j", "ie", "m", "ao"]
    â†“
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: [57, 13, 20, 10, 56, 28, ...] (TokenTextEncoder)
```

**å®Ÿè£…**:
```python
# languages/zh_CN/processor.py
class ChineseTextProcessor:
    def __init__(self):
        self.g2p = G2pM()
        self.tokenizer = jieba

    def convert(self, text: str) -> List[str]:
        """
        ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³ç´ åˆ—

        å‡¦ç†:
        1. å¤šéŸ³å­—è£œæ­£ï¼ˆæœ€é•¿ â†’ æœ€å¸¸ï¼‰
        2. jiebaåˆ†ã‹ã¡æ›¸ã
        3. G2pMéŸ³ç´ å¤‰æ›
        4. å£°æ¯ãƒ»éŸ»æ¯åˆ†å‰²
        """
        # å¤šéŸ³å­—è£œæ­£
        text = self._fix_polyphones(text)

        # æ‹¼éŸ³å¤‰æ›
        pinyins = self.g2p(text, tone=True, char_split=True)

        # éŸ³ç´ åˆ†å‰²
        phonemes = []
        for pinyin in pinyins:
            phonemes.extend(self._split_pinyin(pinyin))

        return phonemes

    def _split_pinyin(self, pinyin: str) -> List[str]:
        """
        æ‹¼éŸ³ â†’ å£°æ¯ãƒ»éŸ»æ¯åˆ†å‰²

        "xiao3" â†’ ["x", "iao"]
        """
        # å£°æ¯ãƒªã‚¹ãƒˆ
        SHENMU = ['zh', 'ch', 'sh', 'b', 'p', 'm', 'f', 'd', 't', 'n', 'l',
                  'g', 'k', 'h', 'j', 'q', 'x', 'r', 'z', 'c', 's', 'y', 'w']

        # éŸ»æ¯ãƒªã‚¹ãƒˆ
        YUNMU = ['a', 'ai', 'an', 'ang', 'ao', 'e', 'ei', 'en', 'eng', 'er',
                 'i', 'ia', 'ian', 'iang', 'iao', 'ie', 'in', 'ing', 'iong',
                 'iu', 'o', 'ong', 'ou', 'u', 'ua', 'uai', 'uan', 'uang',
                 'ui', 'un', 'uo', 'v', 'van', 've', 'vn']

        # å£°èª¿é™¤å»
        pinyin_no_tone = re.sub(r'[0-5]', '', pinyin)

        # å£°æ¯ãƒ»éŸ»æ¯ãƒãƒƒãƒãƒ³ã‚°
        for shenmu in SHENMU:
            if pinyin_no_tone.startswith(shenmu):
                yunmu = pinyin_no_tone[len(shenmu):]
                if yunmu in YUNMU:
                    return [shenmu, yunmu]

        # éŸ»æ¯ã®ã¿
        if pinyin_no_tone in YUNMU:
            return [pinyin_no_tone]

        return [pinyin_no_tone]
```

#### æ—¥æœ¬èªãƒ‘ã‚¹ï¼ˆè¨ˆç”»ï¼‰

```
å…¥åŠ›: "ã“ã‚“ã«ã¡ã¯"
    â†“
MeCabå½¢æ…‹ç´ è§£æ: ["ã“ã‚“ã«ã¡", "ã¯"]
    â†“
ã²ã‚‰ãŒãªç¢ºèª: "ã“ã‚“ã«ã¡ã¯"
    â†“
ãƒ­ãƒ¼ãƒå­—å¤‰æ›: "konnichiwa"
    â†“
éŸ³ç´ åˆ†å‰²: ["k", "o", "N", "n", "i", "ch", "i", "w", "a"]
    â†“
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: [25, 30, 40, 25, 12, ...] (TokenTextEncoder)
```

**å®Ÿè£…è¨ˆç”»**:
```python
# languages/ja_JP/processor.py
class JapaneseTextProcessor:
    def __init__(self):
        self.mecab = MeCab.Tagger("-Owakati")
        self.kakasi = pykakasi.kakasi()

    def convert(self, text: str) -> List[str]:
        """
        æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³ç´ åˆ—

        å‡¦ç†:
        1. MeCabå½¢æ…‹ç´ è§£æ
        2. æ¼¢å­— â†’ ã²ã‚‰ãŒãªå¤‰æ›ï¼ˆpykakasiï¼‰
        3. ã²ã‚‰ãŒãª â†’ ãƒ­ãƒ¼ãƒå­—å¤‰æ›ï¼ˆromkanï¼‰
        4. ãƒ­ãƒ¼ãƒå­— â†’ éŸ³ç´ åˆ†å‰²
        """
        # æ¼¢å­— â†’ ã²ã‚‰ãŒãª
        result = self.kakasi.convert(text)
        kana = ''.join([item['hira'] for item in result])

        # ã²ã‚‰ãŒãª â†’ ãƒ­ãƒ¼ãƒå­—
        romaji = romkan.to_roma(kana)

        # ãƒ­ãƒ¼ãƒå­— â†’ éŸ³ç´ åˆ†å‰²
        phonemes = self._split_romaji(romaji)

        return phonemes

    def _split_romaji(self, romaji: str) -> List[str]:
        """
        ãƒ­ãƒ¼ãƒå­— â†’ éŸ³ç´ åˆ†å‰²

        "konnichiwa" â†’ ["k", "o", "N", "n", "i", "ch", "i", "w", "a"]
        """
        phonemes = []
        i = 0
        while i < len(romaji):
            # 2æ–‡å­—éŸ³ç´ ï¼ˆch, sh, tsç­‰ï¼‰
            if i + 1 < len(romaji):
                two_char = romaji[i:i+2]
                if two_char in ['ch', 'sh', 'ts', 'ky', 'gy', 'ny', ...]:
                    phonemes.append(two_char)
                    i += 2
                    continue

            # 1æ–‡å­—éŸ³ç´ 
            phonemes.append(romaji[i])
            i += 1

        # ä¿ƒéŸ³ï¼ˆã£ï¼‰å‡¦ç†: "N" ã«å¤‰æ›
        phonemes = [p if p != 'n' or i == 0 or phonemes[i-1] != 'n' else 'N'
                    for i, p in enumerate(phonemes)]

        return phonemes
```

### Phase 3: MIDIå‡¦ç†

**MIDIæƒ…å ±ãƒ‘ãƒ¼ã‚¹**:
```python
def parse_midi_info(notes: str, durations: str) -> Dict[str, torch.Tensor]:
    """
    MIDIæ–‡å­—åˆ— â†’ Tensorå¤‰æ›

    Args:
        notes: "C4 | D4 | E4"
        durations: "0.5 | 0.5 | 1.0"

    Returns:
        {
            "pitch_midi": tensor([60, 62, 64]),  # MIDIç•ªå·
            "midi_dur": tensor([12, 12, 24]),    # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            "is_slur": tensor([0, 0, 0])         # ã‚¹ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
        }
    """
    note_list = [n.strip() for n in notes.split('|') if n.strip()]
    dur_list = [float(d.strip()) for d in durations.split('|') if d.strip()]

    # MIDIéŸ³å â†’ MIDIç•ªå·å¤‰æ›
    pitch_midi = [note_to_midi(note) for note in note_list]

    # ç§’ â†’ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°å¤‰æ›ï¼ˆhop_size=128, sr=24000ï¼‰
    hop_size = 128
    sample_rate = 24000
    midi_dur = [int(dur * sample_rate / hop_size) for dur in dur_list]

    # ã‚¹ãƒ©ãƒ¼æ¤œå‡ºï¼ˆãƒãƒ¼ãƒˆåã«"~"æ¥å°¾è¾ï¼‰
    is_slur = [1 if note.endswith('~') else 0 for note in note_list]

    return {
        "pitch_midi": torch.LongTensor(pitch_midi),
        "midi_dur": torch.FloatTensor(midi_dur),
        "is_slur": torch.LongTensor(is_slur)
    }

def note_to_midi(note: str) -> int:
    """
    éŸ³å â†’ MIDIç•ªå·

    "C4" â†’ 60, "C#4" â†’ 61, "Db4" â†’ 61
    """
    # éŸ³åãƒ‘ãƒ¼ã‚¹
    match = re.match(r'([A-G])(#|b)?(\d)', note.strip('~'))
    if not match:
        raise ValueError(f"Invalid note: {note}")

    pitch_class, accidental, octave = match.groups()

    # ãƒ™ãƒ¼ã‚¹MIDIç•ªå·ï¼ˆC0 = 12ï¼‰
    base = {
        'C': 0, 'D': 2, 'E': 4, 'F': 5,
        'G': 7, 'A': 9, 'B': 11
    }[pitch_class]

    # å¤‰åŒ–è¨˜å·
    if accidental == '#':
        base += 1
    elif accidental == 'b':
        base -= 1

    # ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–è¨ˆç®—
    midi_num = base + (int(octave) + 1) * 12

    return midi_num
```

### Phase 4: Acousticæ¨è«–

**GaussianDiffusionå‡¦ç†**:
```python
def acoustic_inference(
    phonemes: torch.Tensor,
    pitch_midi: torch.Tensor,
    midi_dur: torch.Tensor,
    is_slur: torch.Tensor
) -> torch.Tensor:
    """
    éŸ³ç´  + MIDI â†’ Mel-spectrogram

    Args:
        phonemes: [T_txt] éŸ³ç´ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        pitch_midi: [T_note] MIDIç•ªå·
        midi_dur: [T_note] ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
        is_slur: [T_note] ã‚¹ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°

    Returns:
        mel: [T_mel, 80] Mel-spectrogram
    """
    # ãƒ¢ãƒ‡ãƒ«æº–å‚™
    model = GaussianDiffusion(...)
    model.eval()

    with torch.no_grad():
        # Forwardæ¨è«–
        output = model(
            txt_tokens=phonemes.unsqueeze(0),  # [1, T_txt]
            pitch_midi=pitch_midi.unsqueeze(0),  # [1, T_note]
            midi_dur=midi_dur.unsqueeze(0),  # [1, T_note]
            is_slur=is_slur.unsqueeze(0),  # [1, T_note]
            infer=True
        )

        mel = output['mel_out']  # [1, T_mel, 80]

    return mel.squeeze(0)  # [T_mel, 80]
```

### Phase 5: Vocoderæ¨è«–

**HiFi-GANå‡¦ç†**:
```python
def vocoder_inference(mel: torch.Tensor, f0: torch.Tensor) -> np.ndarray:
    """
    Mel + F0 â†’ éŸ³å£°æ³¢å½¢

    Args:
        mel: [T, 80] Mel-spectrogram
        f0: [T] F0 contour

    Returns:
        wav: [T_wav] éŸ³å£°æ³¢å½¢
    """
    # Vocoderæº–å‚™
    vocoder = HifiGAN(...)
    vocoder.eval()

    # å…¥åŠ›æ•´å½¢
    mel = mel.transpose(0, 1).unsqueeze(0)  # [1, 80, T]
    f0 = f0.unsqueeze(0)  # [1, T]

    with torch.no_grad():
        wav = vocoder(mel, f0)  # [1, T_wav]

    return wav.squeeze(0).cpu().numpy()
```

---

## è¨€èªåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

### ä¸­å›½èªãƒ•ãƒ­ãƒ¼

```
"å°é…’çª"
  â†“ (jieba)
["å°", "é…’çª"]
  â†“ (G2pM)
["xiao3", "jiu3", "wo1"]
  â†“ (split)
["x", "iao", "j", "iu", "w", "o"]
  â†“ (encode)
[57, 13, 20, 10, 56, 28]
  â†“ (+ MIDI)
Acoustic Model â†’ Mel
  â†“
Vocoder â†’ WAV
```

### æ—¥æœ¬èªãƒ•ãƒ­ãƒ¼ï¼ˆè¨ˆç”»ï¼‰

```
"æ¡œ"
  â†“ (MeCab + pykakasi)
"ã•ãã‚‰"
  â†“ (romkan)
"sakura"
  â†“ (split)
["s", "a", "k", "u", "r", "a"]
  â†“ (encode)
[35, 1, 25, 32, 34, 1]
  â†“ (+ MIDI)
Acoustic Model â†’ Mel
  â†“
Vocoder â†’ WAV
```

---

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµŒè·¯

```mermaid
graph TD
    A[Request] --> B{Validation}
    B -->|OK| C[Process]
    B -->|NG| E1[400 Bad Request]

    C --> D{Language Supported?}
    D -->|No| E2[404 Language Not Found]
    D -->|Yes| F[Text Processing]

    F --> G{Phoneme Conversion}
    G -->|Fail| E3[500 Conversion Error]
    G -->|Success| H[MIDI Processing]

    H --> I{MIDI Valid?}
    I -->|No| E4[400 Invalid MIDI]
    I -->|Yes| J[Inference]

    J --> K{Model Error?}
    K -->|Yes| E5[500 Inference Error]
    K -->|No| L[Success]

    style E1 fill:#F44336,color:#fff
    style E2 fill:#F44336,color:#fff
    style E3 fill:#F44336,color:#fff
    style E4 fill:#F44336,color:#fff
    style E5 fill:#F44336,color:#fff
    style L fill:#4CAF50,color:#fff
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒãƒƒãƒå‡¦ç†

```python
async def synthesize_batch(requests: List[SynthesisRequest]) -> List[bytes]:
    """
    ãƒãƒƒãƒåˆæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰

    è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åŒæ™‚å‡¦ç†ã—ã¦ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
    """
    tasks = [synthesize_single(req) for req in requests]
    results = await asyncio.gather(*tasks)
    return results
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def convert_text_to_phonemes(text: str, language: str) -> Tuple[str, ...]:
    """
    éŸ³ç´ å¤‰æ›çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥

    åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã®å†å¤‰æ›ã‚’å›é¿
    """
    processor = get_processor(language)
    phonemes = processor.convert(text)
    return tuple(phonemes)  # ãƒªã‚¹ãƒˆ â†’ ã‚¿ãƒ—ãƒ«ï¼ˆhashableï¼‰
```

---

**ä½œæˆè€…**: Claude Code
**ãƒ¬ãƒ“ãƒ¥ãƒ¼**: æœª
**æ‰¿èª**: æœª
