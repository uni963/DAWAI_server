# L3: AI Integration Hub - AIçµ±åˆãƒãƒ–è©³ç´°è¨­è¨ˆ

**æ–‡æ›¸ID**: `DAWAI-ARCH-L3-AI-001`
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0
**æœ€çµ‚æ›´æ–°**: 2025-01-22
**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**:
  - `backend/ai_agent/main.py` (1389è¡Œ)
  - `frontend/src/utils/aiAgentEngine.js` (ç´„800è¡Œ - è¦ç¢ºèª)
**é–¢é€£è¦ä»¶**: REQ-AI-001, REQ-AI-002, REQ-AI-003

## ğŸ“‹ æ¦‚è¦

### ç›®çš„
è¤‡æ•°AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼(Claude, OpenAI, Gemini)ã‚’çµ±åˆã—ã€éŸ³æ¥½åˆ¶ä½œã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹çµ±åˆãƒãƒ–ã®è©³ç´°è¨­è¨ˆã€‚

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æˆ¦ç•¥
- **Strategy Pattern**: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥APIå‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯
- **Factory Pattern**: ãƒ¢ãƒ‡ãƒ«è¨­å®šã«åŸºã¥ãAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
- **Adapter Pattern**: å„AI APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åŒ–
- **Observer Pattern**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•å‡¦ç†

### ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ä¸€è¦§
```yaml
Anthropic Claude:
  - claude-3-sonnet (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
  - claude-3-opus

OpenAI:
  - gpt-4
  - gpt-3.5-turbo

Google Gemini:
  - gemini-2.5-pro
  - gemini-2.5-flash
  - gemini-1.5-pro
  - gemini-1.5-flash
```

---

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### å…¨ä½“æ§‹æˆå›³

```mermaid
graph TB
    subgraph Frontend [ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: React]
        UI[AI Chat UI] --> AE[aiAgentEngine.js]
        AE --> |HTTP POST| API[FastAPI Endpoints]
    end

    subgraph Backend [ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: FastAPI]
        API --> Router["/api/chat<br/>/api/agent<br/>/api/stream/*"]
        Router --> AMM[AIModelManager]
        Router --> SAMM[StreamingAIModelManager]

        AMM --> |åŒæœŸ| Claude1[Claude API]
        AMM --> |åŒæœŸ| OpenAI1[OpenAI API]
        AMM --> |åŒæœŸ| Gemini1[Gemini API]

        SAMM --> |async stream| Claude2[Claude Streaming]
        SAMM --> |async stream| OpenAI2[OpenAI Streaming]
        SAMM --> |async stream| Gemini2[Gemini Streaming]
    end

    subgraph External [å¤–éƒ¨AI API]
        Claude1 --> Anthropic[Anthropic Claude]
        Claude2 --> Anthropic
        OpenAI1 --> OpenAIAPI[OpenAI API]
        OpenAI2 --> OpenAIAPI
        Gemini1 --> GoogleAPI[Google Gemini]
        Gemini2 --> GoogleAPI
    end

    style Frontend fill:#e3f2fd
    style Backend fill:#f3e5f5
    style External fill:#fff3e0
```

### ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ§‹æˆ

```yaml
ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
  é€šå¸¸: POST /api/chat
  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: POST /api/stream/chat
  äº’æ›: POST /ai/api/chat, /ai/api/stream/chat

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
  é€šå¸¸: POST /api/agent
  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: POST /api/stream/agent
  äº’æ›: POST /ai/api/agent, /ai/api/stream/agent

ãã®ä»–:
  éŸ³æ¥½ç”Ÿæˆ: POST /api/generate
  ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: GET /api/health, /ai/api/health
```

---

## ğŸ”§ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…è©³ç´°

### FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

app = FastAPI(
    title="Melodia Composer Copilot API",
    description="AI-powered music composition assistant API with streaming support",
    version="1.0.0"
)

# CORSè¨­å®š (âš ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦æ”¹å–„)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # Viteé–‹ç™ºã‚µãƒ¼ãƒãƒ¼
        "http://localhost:3000",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:3000"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# APIã‚­ãƒ¼ç®¡ç†
DEFAULT_API_KEYS = {
    "anthropic": os.getenv("ANTHROPIC_API_KEY"),
    "openai": os.getenv("OPENAI_API_KEY"),
    "google": os.getenv("GEMINI_API_KEY")
}
```

### ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«

```mermaid
classDiagram
    class ChatRequest {
        +str message
        +Any context
        +str model
        +Dict~str,str~ apiKeys
        +bool stream
    }

    class AgentRequest {
        +str prompt
        +Any context
        +str model
        +str apiKey
        +bool stream
    }

    class ChatResponse {
        +str response
        +bool success
        +str error
    }

    class AgentResponse {
        +List~Dict~ actions
        +str summary
        +str nextSteps
        +bool success
        +str error
    }

    ChatRequest --> ChatResponse: /api/chat
    AgentRequest --> AgentResponse: /api/agent
```

---

## ğŸ¤– AIModelManager ã‚¯ãƒ©ã‚¹ (åŒæœŸå‡¦ç†)

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

```python
class AIModelManager:
    def __init__(self):
        self.default_api_keys = DEFAULT_API_KEYS

    def get_api_key(self, provider: str, custom_keys: Optional[Dict[str, str]] = None) -> Optional[str]:
        """APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼å„ªå…ˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        if custom_keys and provider in custom_keys and custom_keys[provider]:
            return custom_keys[provider]
        return self.default_api_keys.get(provider)

    async def call_claude(self, message: str, context: Any = "", api_key: str = None) -> str:
        """Claude APIã‚’å‘¼ã³å‡ºã—"""
        # å®Ÿè£…è©³ç´°ã¯å¾Œè¿°

    async def call_openai(self, message: str, context: Any = "", api_key: str = None) -> str:
        """OpenAI APIã‚’å‘¼ã³å‡ºã—"""
        # å®Ÿè£…è©³ç´°ã¯å¾Œè¿°

    async def call_gemini(self, message: str, context: Any = "", api_key: str = None) -> str:
        """Gemini APIã‚’å‘¼ã³å‡ºã—"""
        # å®Ÿè£…è©³ç´°ã¯å¾Œè¿°
```

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰æˆ¦ç•¥

```python
# éŸ³æ¥½åˆ¶ä½œã«ç‰¹åŒ–ã—ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
system_prompt = """ã‚ãªãŸã¯éŸ³æ¥½åˆ¶ä½œã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³æ¥½åˆ¶ä½œã«é–¢ã™ã‚‹è³ªå•ã‚„è¦æ±‚ã«å¯¾ã—ã¦ã€å°‚é–€çš„ã§å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®åˆ†é‡ã«ã¤ã„ã¦è©³ã—ãå›ç­”ã§ãã¾ã™ï¼š
- ä½œæ›²ãƒ»ç·¨æ›²ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
- æ¥½å™¨ã®æ¼”å¥æ–¹æ³•
- éŸ³æ¥½ç†è«–
- DAWã®ä½¿ã„æ–¹
- ãƒŸã‚­ã‚·ãƒ³ã‚°ãƒ»ãƒã‚¹ã‚¿ãƒªãƒ³ã‚°
- éŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ«ã®ç‰¹å¾´
- MIDIç·¨é›†

å›ç­”ã¯æ—¥æœ¬èªã§ã€åˆ†ã‹ã‚Šã‚„ã™ãå…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å‡¦ç†
def build_context_info(context):
    context_info = ""

    if isinstance(context, dict):
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
        if context.get("projectInfo"):
            project = context["projectInfo"]
            context_info += f"\n\nã€ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã€‘\n"
            context_info += f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project.get('name', 'Unknown')}\n"
            context_info += f"ãƒ†ãƒ³ãƒ: {project.get('tempo', 'Unknown')} BPM\n"
            context_info += f"ã‚­ãƒ¼: {project.get('key', 'Unknown')}\n"
            # ... ä»–ã®æƒ…å ±

        # ç¾åœ¨ã®ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±
        if context.get("currentTrack"):
            track = context["currentTrack"]
            context_info += f"\nã€ç¾åœ¨é¸æŠä¸­ã®ãƒˆãƒ©ãƒƒã‚¯ã€‘\n"
            context_info += f"ãƒˆãƒ©ãƒƒã‚¯å: {track.get('name', 'Unknown')}\n"
            # ... ä»–ã®æƒ…å ±

        # ä¼šè©±å±¥æ­´
        if context.get("chatHistory"):
            context_info += f"\nã€ä¼šè©±å±¥æ­´ã€‘\n{context['chatHistory']}"

    return context_info
```

### Claude API çµ±åˆ

```python
async def call_claude(self, message: str, context: Any = "", api_key: str = None) -> str:
    if not api_key:
        raise ValueError("Claude API key is required")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    context_info = build_context_info(context)
    full_prompt = system_prompt + context_info + f"\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {message}"

    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01"
    }

    data = {
        "model": "claude-3-sonnet-20240229",
        "max_tokens": 1000,
        "messages": [
            {"role": "user", "content": full_prompt}
        ]
    }

    response = requests.post(
        "https://api.anthropic.com/v1/messages",
        headers=headers,
        json=data,
        timeout=30
    )

    if response.status_code == 200:
        result = response.json()
        return result["content"][0]["text"]
    else:
        raise Exception(f"Claude API error: {response.status_code} - {response.text}")
```

---

## ğŸŒŠ StreamingAIModelManager ã‚¯ãƒ©ã‚¹ (éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°)

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
sequenceDiagram
    participant Client as ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
    participant FastAPI
    participant SAMM as StreamingAIModelManager
    participant API as AI API (Claude/OpenAI/Gemini)

    Client->>FastAPI: POST /api/stream/chat
    FastAPI->>SAMM: stream_claude/openai/gemini()
    SAMM->>API: HTTP POST with stream=true

    loop ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å—ä¿¡
        API-->>SAMM: data: {"type": "text", "content": "..."}
        SAMM-->>FastAPI: yield SSE chunk
        FastAPI-->>Client: data: {"type": "text", "content": "..."}
    end

    API-->>SAMM: data: [DONE]
    SAMM-->>FastAPI: yield data: [DONE]
    FastAPI-->>Client: data: [DONE]
```

### Claude ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…

```python
async def stream_claude(self, message: str, context: Any = "", api_key: str = None):
    """Claude APIã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å‘¼ã³å‡ºã—"""
    print(f"StreamingAIModelManager: Starting Claude streaming for message: {message[:50]}...")

    if not api_key:
        raise ValueError("Claude API key is required")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    context_info = build_context_info(context)
    full_prompt = system_prompt + context_info + f"\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {message}"

    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01"
    }

    data = {
        "model": "claude-3-sonnet-20240229",
        "max_tokens": 1000,
        "messages": [
            {"role": "user", "content": full_prompt}
        ],
        "stream": True  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data
        ) as response:
            if response.status == 200:
                async for line in response.content:
                    if line:
                        line_str = line.decode('utf-8').strip()

                        if line_str.startswith('data: '):
                            data_str = line_str[6:]  # 'data: ' ã‚’é™¤å»

                            if data_str == '[DONE]':
                                yield f"data: [DONE]\n\n"
                                break

                            try:
                                json_data = json.loads(data_str)

                                if 'content' in json_data and json_data['content']:
                                    for content in json_data['content']:
                                        if content.get('type') == 'text':
                                            text = content.get('text', '')
                                            if text:
                                                yield f"data: {json.dumps({'type': 'text', 'content': text})}\n\n"
                            except json.JSONDecodeError as e:
                                print(f"JSON decode error: {e}")
                                continue
            else:
                error_text = await response.text()
                yield f"data: {json.dumps({'type': 'error', 'content': f'Claude API error: {response.status} - {error_text}'})}\n\n"
```

### Gemini ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£… (ç–‘ä¼¼ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°)

```python
async def stream_gemini(self, message: str, context: Any = "", api_key: str = None):
    """Gemini APIã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å‘¼ã³å‡ºã—"""
    if not api_key:
        raise ValueError("Gemini API key is required")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    context_info = build_context_info(context)
    full_prompt = system_prompt + context_info + f"\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {message}"

    # Gemini APIã®è¨­å®š
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-pro')

    try:
        # Gemini APIã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éå¯¾å¿œã®ãŸã‚ç–‘ä¼¼å®Ÿè£…
        response = model.generate_content(full_prompt)
        response_text = response.text

        # æ–‡å­—å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¢¨ã«è¿”ã™
        for i, char in enumerate(response_text):
            yield f"data: {json.dumps({'type': 'text', 'content': char})}\n\n"
            # å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ„Ÿã‚’æ¼”å‡º
            await asyncio.sleep(0.01)

        yield f"data: [DONE]\n\n"

    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'content': f'Gemini API error: {str(e)}'})}\n\n"
```

---

## ğŸ¯ Agent Mode (Sense-Plan-Act ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)

### Agent Mode ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ 

```python
def generate_agent_prompt(user_prompt: str, context: dict) -> str:
    """Sense-Plan-Actã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãAgent modeç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®æ§‹ç¯‰
    context_info = ""
    if context:
        if context.get('currentTrack'):
            track = context['currentTrack']
            context_info += f"\nç¾åœ¨ã®ãƒˆãƒ©ãƒƒã‚¯: {track.get('name')} (ID: {track.get('id')})"

        if context.get('existingTracks'):
            tracks = context['existingTracks']
            context_info += f"\næ—¢å­˜ã®ãƒˆãƒ©ãƒƒã‚¯ ({len(tracks)}å€‹):"
            for track in tracks:
                context_info += f"\n- {track.get('name')} (ID: {track.get('id')}, ã‚¿ã‚¤ãƒ—: {track.get('type')})"

    prompt = f"""ã‚ãªãŸã¯éŸ³æ¥½åˆ¶ä½œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚Sense-Plan-Actã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¾“ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’ç†è§£ã—ã€é©åˆ‡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

## Senseï¼ˆç†è§£ï¼‰æ®µéš
ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçŠ¶æ³ã‚’ç†è§£ã—ã¦ãã ã•ã„ï¼š
{context_info}

## Planï¼ˆè¨ˆç”»ï¼‰æ®µéš
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’åˆ†æã—ã€å®Ÿè¡Œå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç”»ã—ã¦ãã ã•ã„ï¼š
- ãƒˆãƒ©ãƒƒã‚¯ã®è¿½åŠ /ç·¨é›†/å‰Šé™¤
- MIDIãƒãƒ¼ãƒˆã®è¿½åŠ /ç·¨é›†/å‰Šé™¤
- ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã®é©ç”¨
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã®å¤‰æ›´

## Actï¼ˆå®Ÿè¡Œï¼‰æ®µéš
è¨ˆç”»ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "actions": [
    {{
      "type": "æ“ä½œã‚¿ã‚¤ãƒ—",
      "params": {{
        "trackId": "æ­£ç¢ºãªãƒˆãƒ©ãƒƒã‚¯ID",
        "notes": [
          {{
            "id": "note-123",
            "pitch": 60,        // MIDIãƒãƒ¼ãƒˆç•ªå·ï¼ˆ0-127ï¼‰
            "time": 0,          // é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
            "duration": 0.5,    // æŒç¶šæ™‚é–“ï¼ˆç§’ï¼‰
            "velocity": 0.8     // éŸ³é‡ï¼ˆ0-1ï¼‰
          }}
        ]
      }},
      "description": "å®Ÿè¡Œã™ã‚‹æ“ä½œã®èª¬æ˜"
    }}
  ],
  "summary": "å®Ÿè¡Œã—ãŸæ“ä½œã®è¦ç´„",
  "nextSteps": "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆ"
}}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: {user_prompt}"""

    return prompt
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹å‡¦ç†

```python
def parse_agent_response(response_text: str, context: dict) -> dict:
    """Agent modeã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
    import re

    def remove_json_comments(text):
        # // ä»¥é™ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
        return re.sub(r'//.*', '', text)

    try:
        # JSONã®æŠ½å‡ºã‚’è©¦è¡Œ
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            json_str = json_match.group(0)
            json_str = remove_json_comments(json_str)  # ã‚³ãƒ¡ãƒ³ãƒˆé™¤å»
            parsed = json.loads(json_str)

            return {
                "actions": parsed.get("actions", []),
                "summary": str(parsed.get("summary", "æ“ä½œãŒå®Œäº†ã—ã¾ã—ãŸ")),
                "nextSteps": str(parsed.get("nextSteps", "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"))
            }
    except Exception as e:
        print(f"Failed to parse agent response: {e}")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨æ¸¬
    response_lower = response_text.lower()

    if any(word in response_lower for word in ["ãƒˆãƒ©ãƒƒã‚¯", "track", "è¿½åŠ ", "add"]):
        return {
            "actions": [{
                "type": "addTrack",
                "params": {
                    "instrument": "Piano",
                    "trackName": "æ–°ã—ã„ãƒˆãƒ©ãƒƒã‚¯"
                },
                "description": "æ–°ã—ã„ãƒˆãƒ©ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ"
            }],
            "summary": "æ–°ã—ã„ãƒˆãƒ©ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ",
            "nextSteps": "ãƒˆãƒ©ãƒƒã‚¯ã«MIDIãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        }

    # ... ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
```

---

## ğŸµ éŸ³æ¥½ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### éŸ³æ¥½è¦ç´ ç”Ÿæˆãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    A[POST /api/generate] --> B[GenerateRequest]
    B --> C{APIã‚­ãƒ¼å­˜åœ¨?}
    C -->|No| D[HTTP 400 Error]
    C -->|Yes| E[Gemini APIè¨­å®š]

    E --> F[éŸ³æ¥½ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰]
    F --> G["""<br/>type: drum_pattern/bassline/chord_progression/melody/harmony<br/>notes: [pitch, start, duration, velocity]<br/>description: ...<br/>"""]

    G --> H[genai.GenerativeModel.generate_content]
    H --> I[ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONæŠ½å‡º]

    I --> J{JSONè§£ææˆåŠŸ?}
    J -->|Yes| K[GenerateResponseè¿”å´]
    J -->|No| L[ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³<br/>C majorã‚¹ã‚±ãƒ¼ãƒ«]

    K --> M[ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§éŸ³å£°ç”Ÿæˆ]
    L --> M

    style F fill:#e1f5fe
    style G fill:#f3e5f5
    style K fill:#c8e6c9
```

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰
```python
@app.post("/api/generate", response_model=GenerateResponse)
async def generate_music(request: GenerateRequest):
    try:
        api_key = request.apiKey or DEFAULT_API_KEYS.get("google")
        if not api_key:
            raise HTTPException(status_code=400, detail="API key is required")

        genai.configure(api_key=api_key)
        temp_model = genai.GenerativeModel('gemini-2.5-flash')

        music_prompt = f"""
ã‚ãªãŸã¯éŸ³æ¥½åˆ¶ä½œAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€éŸ³æ¥½è¦ç´ ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤º]
{request.prompt}

[å‡ºåŠ›å½¢å¼]
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
  "type": "drum_pattern"|"bassline"|"chord_progression"|"melody"|"harmony",
  "notes": [
    {{"pitch": 60, "start": 0.0, "duration": 0.25, "velocity": 100}},
    ...
  ],
  "description": "ç”Ÿæˆã—ãŸéŸ³æ¥½è¦ç´ ã®èª¬æ˜",
  "suggestions": "è¿½åŠ ã®ææ¡ˆã‚„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³"
}}

éŸ³æ¥½ç†è«–çš„ã«æ­£ã—ãã€æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åˆã£ãŸéŸ³æ¥½è¦ç´ ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""

        response = temp_model.generate_content(music_prompt)

        # JSONã®æŠ½å‡ºã‚’è©¦è¡Œ
        json_match = re.search(r'\{[\s\S]*\}', response.text)
        if json_match:
            json_str = json_match.group(0)
            try:
                parsed = json.loads(json_str)
                return GenerateResponse(
                    type=parsed.get("type", "melody"),
                    notes=parsed.get("notes", []),
                    description=parsed.get("description", "Generated music pattern"),
                    suggestions=parsed.get("suggestions", "")
                )
            except json.JSONDecodeError:
                pass

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ (C major scale)
        fallback_notes = [
            {"pitch": 60, "start": 0.0, "duration": 0.25, "velocity": 80},
            {"pitch": 62, "start": 0.25, "duration": 0.25, "velocity": 80},
            {"pitch": 64, "start": 0.5, "duration": 0.25, "velocity": 80},
            {"pitch": 65, "start": 0.75, "duration": 0.25, "velocity": 80}
        ]

        return GenerateResponse(
            type="melody",
            notes=fallback_notes,
            description="Simple C major scale pattern",
            suggestions="Try different scales or rhythms"
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸŒ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆ (aiAgentEngine.js)

### ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

```javascript
// frontend/src/utils/aiAgentEngine.js (æ¨å®šå®Ÿè£…)
class AIAgentEngine {
  constructor() {
    this.baseURL = import.meta.env.VITE_AI_API_URL || 'http://localhost:8000'
    this.apiKeys = {
      anthropic: localStorage.getItem('anthropic_api_key'),
      openai: localStorage.getItem('openai_api_key'),
      google: localStorage.getItem('google_api_key')
    }
  }

  // é€šå¸¸ãƒãƒ£ãƒƒãƒˆ
  async chat(message, context, model = 'claude-3-sonnet') {
    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        message,
        context,
        model,
        apiKeys: this.apiKeys
      })
    })

    if (!response.ok) {
      throw new Error(`Chat API error: ${response.status}`)
    }

    return await response.json()
  }

  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
  async streamChat(message, context, model, onChunk, onDone, onError) {
    try {
      const response = await fetch(`${this.baseURL}/api/stream/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message,
          context,
          model,
          apiKeys: this.apiKeys
        })
      })

      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ''

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ''

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)
            if (data === '[DONE]') {
              onDone()
              return
            }

            try {
              const parsed = JSON.parse(data)
              if (parsed.type === 'text') {
                onChunk(parsed.content)
              } else if (parsed.type === 'error') {
                onError(parsed.content)
              }
            } catch (e) {
              console.error('Failed to parse SSE data:', e)
            }
          }
        }
      }
    } catch (error) {
      onError(error.message)
    }
  }
}
```

### React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆ

```javascript
// App.jsx ã§ã®åˆ©ç”¨ä¾‹
import AIAgentEngine from './utils/aiAgentEngine.js'

const App = () => {
  const [aiResponse, setAiResponse] = useState('')
  const [isStreaming, setIsStreaming] = useState(false)
  const aiEngine = new AIAgentEngine()

  const handleAIChat = async (message) => {
    setIsStreaming(true)
    setAiResponse('')

    await aiEngine.streamChat(
      message,
      {
        projectInfo: currentProject,
        currentTrack: selectedTrack,
        chatHistory: chatHistory.slice(-5)  // ç›´è¿‘5ä»¶
      },
      'claude-3-sonnet',
      (chunk) => {
        setAiResponse(prev => prev + chunk)  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
      },
      () => {
        setIsStreaming(false)  // å®Œäº†
      },
      (error) => {
        console.error('AI error:', error)
        setIsStreaming(false)
      }
    )
  }

  return (
    <div>
      <ChatInput onSubmit={handleAIChat} />
      <ChatResponse text={aiResponse} isLoading={isStreaming} />
    </div>
  )
}
```

---

## ğŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### API ã‚­ãƒ¼ç®¡ç†

```yaml
ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹ (å„ªå…ˆ):
  - ANTHROPIC_API_KEY
  - OPENAI_API_KEY
  - GEMINI_API_KEY

ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é€ä¿¡ (ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼):
  - ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã® apiKeys ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
  - localStorageä¿å­˜ (âš ï¸ XSSè„†å¼±æ€§ãƒªã‚¹ã‚¯)

ã‚­ãƒ¼å–å¾—å„ªå…ˆé †ä½:
  1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼
  2. ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼
  3. ã‚¨ãƒ©ãƒ¼è¿”å´ (ã‚­ãƒ¼æœªè¨­å®š)
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Ÿè£…

```python
# APIã‚­ãƒ¼æœªè¨­å®šã‚¨ãƒ©ãƒ¼
if not api_key:
    return ChatResponse(
        response=f"{config['provider'].title()} APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šç”»é¢ã§APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        success=False,
        error=f"{config['provider'].title()} API key not configured"
    )

# APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼
try:
    response_text = await ai_manager.call_claude(request.message, request.context, api_key)
    return ChatResponse(response=response_text, success=True)
except Exception as e:
    return ChatResponse(
        response="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        success=False,
        error=str(e)
    )

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ (é–‹ç™ºç’°å¢ƒãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
async def dev_fallback_generator():
    provider_name = config["provider"].title()
    yield f"data: {json.dumps({'type': 'error', 'content': f'{provider_name} APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'})}\n\n"
    yield f"data: {json.dumps({'type': 'info', 'content': 'é–‹ç™ºç’°å¢ƒã§ã¯.envãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚'})}\n\n"
    yield f"data: {json.dumps({'type': 'done'})}\n\n"
```

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

```python
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
response = requests.post(
    "https://api.anthropic.com/v1/messages",
    headers=headers,
    json=data,
    timeout=30  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
)

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éåŒæœŸå‡¦ç†
async with aiohttp.ClientSession() as session:
    async with session.post(url, headers=headers, json=data) as response:
        async for line in response.content:
            # é€æ¬¡å‡¦ç†ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            yield process_line(line)
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ (æ¤œè¨ä¸­)

```yaml
å®Ÿè£…äºˆå®š:
  - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šåŒ–
  - é »å‡ºã‚¯ã‚¨ãƒªã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

æŠ€è¡“é¸æŠè‚¢:
  - Redis: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
  - LocalStorage: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰æ°¸ç¶šåŒ–
  - In-Memory Cache: çŸ­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
```

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°

### ãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

```python
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
@app.get("/api/health")
async def health():
    return {
        "status": "healthy",
        "supported_models": [
            "claude-3-sonnet", "claude-3-opus",
            "gpt-4", "gpt-3.5-turbo",
            "gemini-2.5-pro", "gemini-2.5-flash"
        ]
    }

# ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
print(f"StreamingAIModelManager: Starting Claude streaming for message: {message[:50]}...")
print(f"Chat Debug Info: model={request.model}, apiKeys={request.apiKeys}")
print(f"Chat API Key Result: {api_key[:10] if api_key else 'None'}...")
```

### æ‰‹å‹•ãƒ†ã‚¹ãƒˆæ‰‹é †

```bash
# 1. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰èµ·å‹•
cd backend/ai_agent
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 2. é€šå¸¸ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "C majorã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¡ãƒ­ãƒ‡ã‚£ã‚’ä½œã£ã¦",
    "context": {},
    "model": "claude-3-sonnet",
    "apiKeys": {"anthropic": "sk-ant-..."}
  }'

# 3. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
curl -X POST http://localhost:8000/api/stream/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ãƒ™ãƒ¼ãƒˆãƒ¼ãƒ™ãƒ³é¢¨ã®ãƒ”ã‚¢ãƒãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ææ¡ˆã—ã¦",
    "model": "gemini-2.5-pro",
    "apiKeys": {"google": "AIza..."}
  }'

# 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
curl -X POST http://localhost:8000/api/agent \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ãƒ”ã‚¢ãƒãƒˆãƒ©ãƒƒã‚¯ã«Cãƒ¡ã‚¸ãƒ£ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¦",
    "context": {
      "existingTracks": [
        {"id": "track-1", "name": "Piano", "type": "piano"}
      ]
    },
    "model": "claude-3-sonnet",
    "apiKey": "sk-ant-..."
  }'
```

---

## ğŸ“ˆ ä»Šå¾Œã®æ‹¡å¼µè¨ˆç”»

### Phase 1: é«˜åº¦ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç† (äºˆå®š)
```yaml
è¿½åŠ æ©Ÿèƒ½:
  - é•·æœŸä¼šè©±å±¥æ­´ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åŒ–
  - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æŠ½å‡º
  - ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿å­¦ç¿’ãƒ»ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³

æŠ€è¡“è¦ä»¶:
  - Vector DBçµ±åˆ (Pinecone/Weaviateæ¤œè¨)
  - ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šåŒ– (Redis/PostgreSQL)
  - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æœ€é©åŒ–
```

### Phase 2: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI (æ¤œè¨ä¸­)
```yaml
æ¤œè¨æ©Ÿèƒ½:
  - éŸ³å£°å…¥åŠ›ãƒ»å‡ºåŠ› (Whisper APIçµ±åˆ)
  - æ¥½è­œç”»åƒèªè­˜ (Vision API)
  - MIDIè‡ªå‹•æ¡è­œãƒ»è£œæ­£

æŠ€è¡“é¸æŠè‚¢:
  - OpenAI Whisper: éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆ
  - Claude Vision: æ¥½è­œè§£æ
  - Magenta: MIDIç”Ÿæˆãƒ»å¤‰æ›
```

---

## ğŸ“ ã¾ã¨ã‚

### å®Ÿè£…å®Œæˆåº¦
- **ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½**: 100% (3ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…)
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½**: 90% (JSONè§£æã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…æ¸ˆã¿)
- **éŸ³æ¥½ç”Ÿæˆ**: 80% (Geminiçµ±åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³)
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: 85% (APIéšœå®³å¯¾å¿œã€é–‹ç™ºç’°å¢ƒãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: 70% (CORSè¦æ”¹å–„ã€APIã‚­ãƒ¼ç®¡ç†å¼·åŒ–å¿…è¦)

### ä¸»è¦ãªæŠ€è¡“çš„æˆæœ
1. **ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµ±åˆ**: 3ç¤¾AI APIã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åŒ–
2. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¡¨ç¤º
3. **éŸ³æ¥½ç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜çµ„ã¿è¾¼ã¿
4. **Sense-Plan-Act**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿè£…

### æ—¢çŸ¥ã®åˆ¶é™äº‹é …
```yaml
åˆ¶é™1: CORSè¨­å®š
  ç¾çŠ¶: allow_origins=["*"] (é–‹ç™ºç”¨)
  å½±éŸ¿: æœ¬ç•ªç’°å¢ƒã§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯
  å¯¾ç­–: ç’°å¢ƒåˆ¥è¨­å®šã€ã‚ªãƒªã‚¸ãƒ³æ¤œè¨¼å¼·åŒ–

åˆ¶é™2: APIã‚­ãƒ¼ç®¡ç†
  ç¾çŠ¶: ç’°å¢ƒå¤‰æ•° + ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
  å½±éŸ¿: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚­ãƒ¼éœ²å‡ºãƒªã‚¹ã‚¯
  å¯¾ç­–: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å°‚ç”¨ã‚­ãƒ¼ç®¡ç†

åˆ¶é™3: ãƒ¬ãƒ¼ãƒˆåˆ¶é™æœªå¯¾å¿œ
  ç¾çŠ¶: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®ã¿
  å½±éŸ¿: APIåˆ¶é™è¶…éæ™‚ã®ã‚¨ãƒ©ãƒ¼
  å¯¾ç­–: ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã€ãƒãƒƒã‚¯ã‚ªãƒ•å®Ÿè£…
```

---

**å‚ç…§å®Ÿè£…**:
- `backend/ai_agent/main.py:1-1389`
- `frontend/src/utils/aiAgentEngine.js` (è¦ç¢ºèª)

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- `specs/requirements/functional/L2_ai_integration/index.md`
- `specs/architecture/logical/L2_backend.md`

**ãƒ†ã‚¹ãƒˆæ‰‹é †**:
1. `cd backend/ai_agent && uvicorn main:app --reload`
2. ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5173 èµ·å‹•
3. AI Chat UIã§å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
